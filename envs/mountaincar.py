# -*- coding: utf-8 -*-
"""
@File    : mountaincar.py      # æ–‡ä»¶åï¼Œmountaincarè¡¨ç¤ºå½“å‰æ–‡ä»¶å
@Time    : 2025/1/10         # åˆ›å»ºæ—¶é—´ï¼Œ2025/1/10è¡¨ç¤ºå½“å‰æ—¶é—´
@Author  : <your_name>     # ä½œè€…
@Email   : <your_email>    # ä½œè€…ç”µå­é‚®ä»¶
@Desc    : <brief_description> # æ–‡ä»¶çš„ç®€è¦æè¿°
"""

import time
from collections import deque
import torch
import os
import keras
from keras.api.initializers import GlorotUniform
import tensorflow as tf
import gym
import numpy as np
import logging
import torch.nn as nn
import pandas as pd
from tqdm import tqdm

from envs.env_template import Env
from tools.visualizer import Visualizer
from tools.save_policy import Policy_loader

logger = logging.getLogger(__name__)  # ä½¿ç”¨å½“å‰æ¨¡å—å
from envs.global_set import *

# åˆ›å»ºå…¨å±€å˜é‡æ¨¡æ‹Ÿå­¦ä¹ é˜¶æ®µ
"""
å°è½¦ä¸Šå±±ç¯å¢ƒè¯´æ˜ï¼š
ä»¥æ°´å¹³æ–¹å‘ä¸ºå‚è€ƒï¼Œä½ç½®é€Ÿåº¦èŒƒå›´éƒ½æ˜¯ä»¥å±±åº•çš„ä¸º0ï¼Œ
å‘å·¦èµ°çš„æ—¶å€™ï¼Œé€Ÿåº¦ä¸ºè´Ÿï¼Œä½ç½®ä¸ºè´Ÿ-[-1.2,0.6]
å‘å³èµ°çš„æ—¶å€™ï¼Œé€Ÿåº¦ä¸ºæ­£ï¼Œä½ç½®ä¸ºæ­£-[-0.07,0.07]
åŠ¨ä½œèŒƒå›´æ–½åŠ›æ–¹å‘ï¼šå‘å·¦ï¼Œæ— ï¼Œå‘å³-[0,1,2]
åˆå§‹ä½ç½®ä¸º[-0.6,-0.4]
åˆå§‹é€Ÿåº¦ï¼š0
"""


class EnvInit(Env):
    """
    ç®—æ³•å‚æ•°åˆå§‹åŒ–
    """

    def __init__(self, name='MountainCar-v0', render_mode=render_model[0], render=True):
        super().__init__(name, render_mode, render)
        # æ˜¯å¦å¼€å¯åŠ¨ç”»
        if render:
            self.env = gym.make(name, render_mode=render_mode)
        else:
            self.env = gym.make(name)

        self.render = render
        # æ¸¸æˆè½®æ•°
        self.game_rounds = 25000
        # è·å–åŠ¨ä½œç©ºé—´çš„å¤§å°ï¼Œå³å¯é€‰æ‹©çš„åŠ¨ä½œæ•°é‡
        self.Action_Num = self.env.action_space.n
        # ä½ç½®
        self.positions = []
        # é€Ÿåº¦
        self.velocities = []
        # ä¿å­˜æ¨¡å‹
        self.save_policy = False
        # åŠ è½½æ¨¡å‹
        self.load_model = True
        # æŠ˜æ‰£å› å­ï¼Œå†³å®šäº†æœªæ¥å¥–åŠ±çš„å½±å“
        self.gamma = 1.
        # å­¦ä¹ ç‡
        self.learning_rate = 0.01
        # æŸ¯è¥¿æ”¶æ•›èŒƒå›´
        self.tolerant = 1e-6
        # Îµ-æŸ”æ€§ç­–ç•¥å› å­
        self.epsilon = 0.001
        self.translate_action = {
            0: "å·¦",
            1: "æ— ",
            2: "å³"
        }

    @property
    def print_env_info(self):
        return self.__env_info

    def __env_info(self):
        logger.info(f'è§‚æµ‹ç©ºé—´ï¼š{self.envs.observation_space}')
        logger.info(f'åŠ¨ä½œç©ºé—´ï¼š{self.envs.action_space}')
        logger.info(f'ä½ç½®èŒƒå›´ï¼š{(self.envs.min_position, self.envs.max_position)}')
        logger.info(f'é€Ÿåº¦èŒƒå›´ï¼š{(-self.envs.max_speed, self.envs.max_speed)}')
        logger.info(f'ç›®æ ‡ä½ç½®ï¼š{self.envs.goal_position}')


class TileCoder:
    """
    ç“¦ç –ç¼–ç ï¼šå¯¹äºå¯ä»¥å®ç°ç›®æ ‡å‡½æ•°çš„çŠ¶æ€ï¼Œå°½å¯èƒ½æ•æ‰ç›¸ä¼¼æ€§ï¼Œå¯¹äºæ— æ³•å®Œæˆç›®æ ‡å‡½æ•°çš„çŠ¶æ€ï¼Œå°½å¯èƒ½åŒºåˆ†å·®å¼‚æ€§
    è¾“å…¥ç‰¹å¾å¯èƒ½æ˜¯ï¼š[ä½ç½®ï¼Œéšœç¢ç‰©ï¼Œç›®æ ‡è·ç¦»ï¼Œæ–¹å‘ä¿¡æ¯]ï¼Œå¯¹åº”çš„æƒé‡ä¹Ÿæœ‰ç›¸åŒç»´åº¦ï¼Œ
    ä½†æ˜¯åœ¨å…·ä½“åˆ†ç»„æ—¶ï¼Œä¼šæ ¹æ®å…·ä½“æƒ…å†µåˆ†é…æƒé‡ï¼Œæ¯”å¦‚éšœç¢ç‰©ï¼Œï¼ˆ1ï¼Œ-10ï¼Œ10ï¼Œ5ï¼‰ï¼Œå°†æ•´ä½“ä»·å€¼æ‹‰ä½
    è€Œåœ¨çº¿æ€§å‡½æ•°ä¸­ä¼šæ ¹æ®çŠ¶æ€çš„æ€»ä½“ä¸åŒæƒ…å†µåˆ†ç»„ï¼Œæ¯”å¦‚é¦–å…ˆæ˜¯éšœç¢ç‰©ï¼Œç„¶åæ˜¯ç›®æ ‡è·ç¦»...ï¼Œä¸ä¼šå¯¹æ‰€æœ‰æƒ…å†µè®¡ç®—æƒé‡ï¼Œ
    è¿™å°±å®ç°äº†æ³›åŒ–ï¼Œå³ç›¸ä¼¼ç‰¹å¾å¯ä»¥ä½¿ç”¨ç›¸åŒæƒé‡

    æœ¬è´¨ï¼šç¼–ç è¿‡ç¨‹å°±æ˜¯å¯¹çœŸå®ä¸–ç•Œç‰©ç†é‡ä¾¿äºä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒè€Œå°†è¿ç»­çš„çŠ¶æ€è½¬åŒ–ä¸ºç¦»æ•£çš„è¡¨ç¤ºï¼Œ
    åœ¨å®šä¹‰åï¼Œæ‰€æœ‰è®­ç»ƒçš„çŠ¶æ€å‘é‡éƒ½åº”è¯¥éµå¾ªè¿™ä¸ªè§„åˆ™ï¼Œåœ¨è®­ç»ƒåï¼Œåœ¨å°†è¾“å‡ºä¼ é€’ç»™ç°å®ä¸–ç•Œè¿›è¡Œå†³ç­–è§„åˆ’ï¼Œ

    ä¼ æ„Ÿå™¨é‡‡æ ·å¾—åˆ°çš„ç‰©ç†ä¿¡æ¯ï¼ˆä¾‹å¦‚ä½ç½®ã€é€Ÿåº¦ã€è§’åº¦ç­‰ï¼‰ä¼šå®æ—¶ä¼ é€’ç»™ç“¦ç –ç½‘ç»œè¿›è¡Œç¼–ç ï¼Œ
    ç“¦ç –ç½‘ç»œè´Ÿè´£å°†è¿™äº›è¿ç»­çš„ç‰©ç†ä¿¡æ¯ç¦»æ•£åŒ–ï¼Œ
    å¹¶é€šè¿‡ç‰¹å®šçš„ç¼–ç æ–¹å¼ï¼ˆä¾‹å¦‚ç“¦ç –ç¼–ç ã€å“ˆå¸Œç¼–ç ç­‰ï¼‰å°†è¿™äº›ä¿¡æ¯è½¬æ¢ä¸ºé€‚åˆç”¨äºè®­ç»ƒçš„ç‰¹å¾è¡¨ç¤º
    """

    def __init__(self, layers, features, codebook=None):
        self.layers = layers  # ç“¦ç –çš„å±‚æ•°
        self.features = features  # æœ€å¤šèƒ½å¤Ÿå­˜å‚¨çš„ç‰¹å¾æ•°ï¼Œæƒé‡å‚æ•°çš„ç»´åº¦
        self.codebook = codebook if codebook else {}  # ç”¨äºå­˜å‚¨æ¯ä¸ªç¼–ç å¯¹åº”çš„ç‰¹å¾{(0,3,2,3):1,(0,1,2,1):2}

    @property
    def get_features(self):
        return self.__get_features

    def __get_features(self, codeword):
        # codebook = {(0, 25, 10, 1): 0, (0, 25, 10, 2): 1, (0, 25, 11, 1): 2}
        logger.debug(f"codeword:{codeword}")
        codeword = tuple(codeword)
        if codeword in self.codebook:
            return self.codebook[codeword]  # å¦‚æœå·²ç»è®¡ç®—è¿‡è¿™ä¸ªç¼–ç ï¼Œåˆ™è¿”å›å¯¹åº”çš„ç‰¹å¾ID
        # æ¯æ¬¡å¤šä¸ªcodewordï¼Œ+1
        count = len(self.codebook)
        if count >= self.features:
            return hash(codeword) % self.features  # å¦‚æœç‰¹å¾æ•°é‡è¶…å‡ºæœ€å¤§é™åˆ¶ï¼Œè¿›è¡Œå“ˆå¸Œæ˜ å°„ï¼Œ
            # è¯¥hashå°†é‡Œé¢çš„tupleå¤šä¸ªå€¼è®¡ç®—å‡ºä¸€ä¸ªæ•´æ•°ï¼Œå†å–æ¨¡é˜²æ­¢å“ˆå¸Œç¢°æ’
        else:
            self.codebook[codeword] = count  # å¦‚æœç‰¹å¾æ•°é‡æœªè¶…å‡ºé™åˆ¶ï¼Œåˆ™ä¸ºè¯¥ç¼–ç åˆ†é…ä¸€ä¸ªæ–°çš„ç‰¹å¾ID
            return count

    def __call__(self, floats=(), ints=()):
        """
        floats: æµ®åŠ¨ç‰¹å¾ï¼Œç¦»æ•£åŒ–çš„è¿ç»­è¾“å…¥ç‰¹å¾, floats = (3.4, 1.2)

        # åˆ›å»º BrickNetwork ç±»çš„å®ä¾‹ï¼Œå‡è®¾å±‚æ•°ä¸º 3
        network = BrickNetwork(layers=3)
        # è°ƒç”¨å®ä¾‹ï¼Œä¼ å…¥æµ®åŠ¨ç‰¹å¾ (ä½ç½®ã€é€Ÿåº¦) å’Œæ•´æ•°ç‰¹å¾ (ä¾‹å¦‚åŠ¨ä½œ)
        floats = (3.4, 1.2)  # å‡è®¾ä½ç½®æ˜¯ 3.4ï¼Œé€Ÿåº¦æ˜¯ 1.2
        ints = (0,)  # å‡è®¾æ•´æ•°ç‰¹å¾æ˜¯ 0ï¼Œå¯èƒ½ä»£è¡¨æŸä¸ªåŠ¨ä½œ
        # ä½¿ç”¨ __call__ æ–¹æ³•ï¼ˆå®é™…ä¸Šæ˜¯ç›´æ¥é€šè¿‡å®ä¾‹è°ƒç”¨ï¼‰å¾—åˆ°ç¦»æ•£åŒ–çš„ç‰¹å¾
        features = network(floats=floats, ints=ints)
        """
        dim = len(floats)

        # ä¸¾ä¾‹ï¼šå¯¹äºè¾“å…¥ä¸º(0,10)çš„åŒºé—´ï¼Œå¦‚æœè¢«layers=3åˆ’åˆ†ï¼Œä¸”æ¯ä¸ªåˆ’åˆ†çš„åç§»é‡ä¸åŒï¼Œ
        # ä¸åŒçš„ä½¿å¾—æ¯ä¸€å±‚çš„ç“¦ç –åˆ’åˆ†å…·æœ‰ä¸åŒçš„ç²¾åº¦å’Œè§†è§’ï¼Œå› æ­¤å¢å¼ºäº†ç¼–ç çš„è¡¨è¾¾èƒ½åŠ›ã€‚

        # ä¾‹å¦‚ï¼Œå‡è®¾å±‚æ•°
        # m = 3ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šå¯¹ä½ç½®ç‰¹å¾
        # xçš„æ¯ä¸€å±‚ä½¿ç”¨ä¸åŒçš„åç§»é‡ï¼š
        # ç¬¬ä¸€å±‚ï¼šä½ç½®xåˆ’åˆ†ä¸º[0, 3), [3, 6), [6, 9), [9, 10]
        # ç¬¬äºŒå±‚ï¼šä½ç½®xåˆ’åˆ†ä¸º[0, 2), [2, 5), [5, 8), [8, 10]
        # ç¬¬ä¸‰å±‚ï¼šä½ç½®xåˆ’åˆ†ä¸º[0, 1), [1, 4), [4, 7), [7, 10]
        # å¯ä»¥æŠŠç¼©æ”¾çœ‹ä½œæ˜¯é¢ç§¯çš„æ”¾å¤§ï¼Œå› ä¸ºé¢ç§¯æ˜¯x^2ï¼Œå½“xç¼©æ”¾3å€ï¼Œå°±æ˜¯3xï¼Œé¢ç§¯å°±æ˜¯3*3*x^2ï¼Œæ‰€ä»¥ï¼Œæ˜¯å¯¹äºæŸä¸€ä¸ªç‰¹å¾æ˜¯f*layer*layer
        scales_floats = tuple(f * self.layers * self.layers for f in floats)
        features = []
        for layer in range(self.layers):
            # 1 + dim * iç›®çš„æ˜¯ä¸ºäº†åœ¨ä¸åŒçš„å±‚ï¼ˆlayerï¼‰å’Œç‰¹å¾ï¼ˆiï¼‰ä¹‹é—´å¼•å…¥ä¸åŒçš„åç§»é‡ã€‚
            # å½“ i = 0 æ—¶ï¼Œåç§»é‡æ˜¯ 1 + 3 * 0 = 1ï¼Œè¿™å°±ç›¸å½“äºç»™ç¬¬ä¸€ä¸ªç‰¹å¾ï¼ˆæ¯”å¦‚ä½ç½®ï¼‰æ·»åŠ ä¸€ä¸ªåŸºæœ¬çš„åç§»é‡ 1ã€‚
            # å½“ i = 1 æ—¶ï¼Œåç§»é‡æ˜¯ 1 + 3 * 1 = 4ï¼Œè¿™å°±ç›¸å½“äºç»™ç¬¬äºŒä¸ªç‰¹å¾ï¼ˆæ¯”å¦‚é€Ÿåº¦ï¼‰æ·»åŠ ä¸€ä¸ªåç§»é‡ 4ã€‚
            # å½“ i = 2 æ—¶ï¼Œåç§»é‡æ˜¯ 1 + 3 * 2 = 7ï¼Œè¿™å°±ç›¸å½“äºç»™ç¬¬ä¸‰ä¸ªç‰¹å¾ï¼ˆæ¯”å¦‚è§’åº¦ï¼‰æ·»åŠ ä¸€ä¸ªåç§»é‡ 7ã€‚
            # å°†æ¯ä¸€å±‚çš„ç¦»æ•£åŒ–ç‰¹å¾å’Œæ•´æ•°ç‰¹å¾ï¼ˆå¦‚çŠ¶æ€æˆ–åŠ¨ä½œï¼‰ä¸€èµ·æ‹¼æ¥æˆä¸€ä¸ª codeword
            # dimä½œç”¨: å¢å¤§ä¸åŒç‰¹å¾ä¹‹é—´çš„åŒºåˆ«é˜²æ­¢ç‰¹å¾çš„åç§»é‡ç›¸äº’å¹²æ‰°ï¼›ç“¦ç –ç¼–ç çš„è¡¨è¾¾èƒ½åŠ›ä¸‹é™
            codeword = ((layer,) + tuple(int((f + (1 + dim * i) * layer) / self.layers)
                                         for i, f in enumerate(scales_floats)) +
                        (ints if isinstance(ints, tuple) else (ints,)))
            # codeword = (0, 25, 10, 1)
            feature = self.__get_features(codeword)
            # featureåœ¨self.codebookä¸­å¯¹åº”çš„å€¼ï¼Œè¿™ä¸ªæ˜ å°„ç›¸å½“äºåšä¸ªè½¬æ¢ï¼Œå°†ç¼–ç å…ƒç»„è½¬æ¢ä¸ºwä¸­çš„ç´¢å¼•
            features.append(feature)
        return features


class SARSAAgent(EnvInit):
    """
    å‡½æ•°è¿‘ä¼¼SARSAç®—æ³•
    1. ç»´åº¦æ•°é‡é—®é¢˜
    2. çŠ¶æ€ç¼–ç è¿‡ç¨‹
    3. è®­ç»ƒè¿‡ç¨‹ä»¥åŠç®—æ³•æ›´æ–°è¿‡ç¨‹
    """

    def __init__(self, layers=8, features=1893):
        """
        åˆå§‹åŒ–SARSA Agent
        :param layers: TileCoderçš„å±‚æ•°ï¼ˆå¤šå±‚ç¼–ç ç”¨äºæ›´ç»†ç²’åº¦çš„çŠ¶æ€è¡¨ç¤ºï¼‰
        :param features: æ€»çš„ç‰¹å¾æ•°é‡
        """
        super().__init__()  # åˆå§‹åŒ–çˆ¶ç±»ï¼ˆåŒ…å«ç¯å¢ƒç›¸å…³å‚æ•°ï¼‰
        self.obs_low = self.env.observation_space.low  # ç¯å¢ƒè§‚æµ‹çš„æœ€å°å€¼
        self.obs_scale = self.env.observation_space.high - self.env.observation_space.low  # ç¯å¢ƒè§‚æµ‹çš„èŒƒå›´
        self.layers = layers  # TileCoder çš„å±‚æ•°
        self.features = features  # ç‰¹å¾æ•°é‡

        if not self.load_model:  # å¦‚æœæœªåŠ è½½æ¨¡å‹ï¼Œåˆ™åˆå§‹åŒ– TileCoder å’Œæƒé‡
            self.tile_coder = TileCoder(layers, features)  # åˆå§‹åŒ–TileCoderï¼Œç”¨äºçŠ¶æ€å’ŒåŠ¨ä½œçš„ç¼–ç 
            self.weights = np.zeros(features)  # åˆå§‹åŒ–æƒé‡ä¸ºé›¶å‘é‡ï¼ŒæŠŠweightsçœ‹ä½œæ˜¯Q-table
        else:  # å¦‚æœåŠ è½½æ¨¡å‹ï¼Œåˆ™æ¢å¤æƒé‡å’Œç¼–ç å™¨çŠ¶æ€
            self.weights, codebook = Policy_loader.load_w_para(class_name=self.__class__.__name__,
                                                               method_name="play_game_by_sarsa_resemble.pkl")
            self.tile_coder = TileCoder(layers, features, codebook)  # ä½¿ç”¨åŠ è½½çš„codebookåˆå§‹åŒ–TileCoder

    def preprocess_encode(self, observation, action):
        """
        ç¼–ç è§‚æµ‹å’ŒåŠ¨ä½œä¸ºç‰¹å¾å‘é‡
        :param observation: å½“å‰çŠ¶æ€ï¼ˆè¿ç»­å€¼ï¼‰
        :param action: åŠ¨ä½œï¼ˆç¦»æ•£å€¼ï¼‰
        :return: ç‰¹å¾ç´¢å¼•åˆ—è¡¨[]->list
        """
        # å°†è§‚æµ‹å€¼å½’ä¸€åŒ–åˆ° [0, 1] èŒƒå›´ï¼Œå¹¶è½¬æ¢ä¸ºå…ƒç»„
        states = tuple((observation - self.obs_low) / self.obs_scale)
        # å°†åŠ¨ä½œå°è£…ä¸ºå…ƒç»„
        actions = (action,)
        # ä½¿ç”¨TileCoderç¼–ç ä¸ºç‰¹å¾ç´¢å¼•ï¼Œè¯¥ç“¦ç –ç½‘ç»œå°†ï¼ˆstatesï¼Œactionsï¼‰è½¬æ¢ä¸ºç‰¹å¾ç´¢å¼•ï¼ˆåœ¨weightsä¸­çš„ç´¢å¼•ï¼‰
        return self.tile_coder(states, actions)

    def get_weights(self, observation, action):
        """
        æ ¹æ®layerså±‚æ•°è·å–å½“å‰çš„ï¼ˆobservation, actionï¼‰åœ¨ä¸åŒå±‚çš„ç‰¹å¾çš„ç´¢å¼•ï¼Œé€šè¿‡ç´¢å¼•åœ¨wä¸­æ‰¾åˆ°å‚æ•°ï¼Œæ±‚å’Œ
        è·å–åŠ¨ä½œçš„Qå€¼
        :param observation: å½“å‰çŠ¶æ€
        :param action: åŠ¨ä½œ
        :return: å¯¹åº”çš„weightsæˆ–è€…Q(s, a)å€¼
        """
        features = self.preprocess_encode(observation, action)  # ç¼–ç è§‚æµ‹å’ŒåŠ¨ä½œä¸ºç‰¹å¾ç´¢å¼•: [16, 29, 71, 19, 20, 21, 22, 23]
        return self.weights[features].sum()  # æ ¹æ®æƒé‡å’Œç‰¹å¾è®¡ç®—Qå€¼

    def agent_resemble_decide(self, observation):
        """
        æ ¹æ®å½“å‰ç­–ç•¥è¿›è¡ŒåŠ¨ä½œå†³ç­–
        :param observation: å½“å‰çŠ¶æ€
        :return: é€‰å®šçš„åŠ¨ä½œ
        """
        if np.random.rand() < self.epsilon:  # ä»¥epsilonæ¦‚ç‡éšæœºé€‰æ‹©åŠ¨ä½œï¼ˆæ¢ç´¢ï¼‰
            return np.random.randint(self.Action_Num)
        else:  # å¦åˆ™é€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œï¼ˆåˆ©ç”¨ï¼‰
            q_value = [self.get_weights(observation, action) for action in range(self.Action_Num)]
            return np.argmax(q_value)  # è¿”å›Qå€¼æœ€å¤§çš„åŠ¨ä½œç´¢å¼•

    def sarsa_resemble_learn(self, observation, action, reward, next_observation, next_action, done):
        """
        ä½¿ç”¨SARSAæ›´æ–°è§„åˆ™è¿›è¡Œå­¦ä¹ 
        :param observation: å½“å‰çŠ¶æ€
        :param action: å½“å‰åŠ¨ä½œ
        :param reward: å½“å‰å¥–åŠ±
        :param next_observation: ä¸‹ä¸€ä¸ªçŠ¶æ€
        :param next_action: ä¸‹ä¸€ä¸ªåŠ¨ä½œ
        :param done: æ˜¯å¦ä¸ºç»ˆæ­¢çŠ¶æ€
        """
        # è®¡ç®—ç›®æ ‡å€¼u
        u_t = reward + (1. - done) * self.gamma * self.get_weights(next_observation, next_action)
        # TDè¯¯å·®ï¼šç›®æ ‡å€¼ä¸å½“å‰Qå€¼çš„å·®
        td_error = u_t - self.get_weights(observation, action)
        # è·å–å½“å‰çŠ¶æ€å’ŒåŠ¨ä½œçš„ç‰¹å¾ç´¢å¼•
        features = self.preprocess_encode(observation, action)
        # æ ¹æ®TDè¯¯å·®æ›´æ–°æƒé‡
        self.weights[features] += self.learning_rate * td_error

    def play_game_by_sarsa_resemble(self, train=False):
        """
        ä½¿ç”¨SARSAç®—æ³•è®­ç»ƒ
        :param train:
        :return:
        """
        episode_reward = 0
        observation, _ = self.reset()
        action = self.agent_resemble_decide(observation)

        done = False
        while True:
            if self.render:
                self.env.render()

            next_observation, reward, terminated, truncated, _ = self.step(action)
            episode_reward += reward
            # logger.info(f"å½“å‰çŠ¶æ€:{next_observation}")
            next_action = self.agent_resemble_decide(next_observation)
            # logger.info(f"é€‰æ‹©åŠ¨ä½œ:{next_action}")

            if terminated or truncated:
                done = True

            if train:
                self.sarsa_resemble_learn(observation, action, reward, next_observation, next_action, done)
            else:
                time.sleep(2)

            if done:
                logger.info(f"ç»“æŸä¸€è½®æ¸¸æˆ")
                break
            observation, action = next_observation, next_action
        return episode_reward


class SARSALamdaAgent(EnvInit):
    """
    å‡½æ•°è¿‘ä¼¼SARSA(ğœ†)ç®—æ³•
    """

    def __init__(self, lamda=0.9, layers=8, features=1893):
        super().__init__()
        self.lamda = lamda
        self.layers = layers
        self.features = features
        self.obs_low = self.env.observation_space.low
        self.obs_scale = self.observation_space.high - self.env.observation_space.low
        self.e_tracy = np.zeros(features)
        # åŠ è½½æ¨¡å‹
        self.train = True
        self.load_model = True
        if not self.load_model:
            self.tile_coder = TileCoder(self.layers, self.features)
            self.weights = np.zeros(features)
        else:
            self.weights, codebook = Policy_loader.load_w_para(class_name=self.__class__.__name__,
                                                               method_name="play_game_by_sarsa_lamda.pkl")
            self.tile_coder = TileCoder(self.layers, self.features, codebook)

    def process_encode(self, observation, action):
        """
        ç¼–ç 
        """
        states = tuple((observation - self.obs_low) / self.obs_scale)
        actions = (action,)
        return self.tile_coder(states, actions)

    def get_weights(self, observation, action):
        """
        è·å–åŠ¨ä½œä»·å€¼
        :param observation:
        :param action:
        :return:
        """
        features = self.process_encode(observation, action)
        # logger.info(f"features:{features}")
        return self.weights[features].sum()

    def agent_resemble_decide(self, observation):
        """
        å†³ç­–
        :param observation:
        :return:
        """
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.Action_Num)
        else:
            q_value = [self.get_weights(observation, action) for action in range(self.Action_Num)]
            return np.argmax(q_value)

    def sarsa_lamda_learn(self, observation, action, reward, next_observation, next_action, done):
        """
        ä½¿ç”¨ SARSA(Î») ç®—æ³•è¿›è¡Œå­¦ä¹ 
        åœ¨ SARSA(Î») æˆ–å…¶ä»–åŸºäºèµ„æ ¼è¿¹çš„ç®—æ³•ä¸­ï¼Œæ‰€æœ‰çš„æƒé‡éƒ½ä¼šè¢«æ›´æ–°ï¼Œä½†æ˜¯æ¯ä¸ªæƒé‡çš„æ›´æ–°å¹…åº¦æ˜¯ä¸åŒçš„ï¼Œ
        å…·ä½“å–å†³äºå®ƒä»¬å¯¹åº”çš„çŠ¶æ€-åŠ¨ä½œå¯¹åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­çš„ èµ„æ ¼è¿¹ï¼ˆeligibility traceï¼‰ã€‚
        èµ„æ ¼è¿¹åæ˜ äº†æ¯ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹å¯¹å½“å‰è¯¯å·®çš„è´¡çŒ®ç¨‹åº¦ï¼Œä¹Ÿå°±æ˜¯å®ƒåœ¨å†å²ä¸­è¢«è®¿é—®çš„é¢‘ç‡ã€‚
        èµ„æ ¼è¿¹è¶Šå¤§çš„çŠ¶æ€-åŠ¨ä½œå¯¹ï¼Œä¼šå¾—åˆ°æ›´å¤šçš„æ›´æ–°ï¼Œå› ä¸ºå®ƒä»¬åœ¨å†å²ä¸­å¯¹å½“å‰å›æŠ¥çš„å½±å“æ›´å¤§ã€‚

        :param observation: å½“å‰çŠ¶æ€
        :param action: å½“å‰åŠ¨ä½œ
        :param reward: å½“å‰å¥–åŠ±
        :param next_observation: ä¸‹ä¸€ä¸ªçŠ¶æ€
        :param next_action: ä¸‹ä¸€ä¸ªåŠ¨ä½œ
        :param done: æ˜¯å¦ä¸ºç»ˆæ­¢çŠ¶æ€
        """

        # è®¡ç®—å½“å‰çš„ç›®æ ‡å€¼ u_t
        u_t = reward  # å½“å‰å¥–åŠ±ä½œä¸ºåˆå§‹ç›®æ ‡å€¼
        if not done:
            # å¦‚æœä¸æ˜¯ç»ˆæ­¢çŠ¶æ€ï¼Œç›®æ ‡å€¼ä¸­éœ€è¦åŠ å…¥ä¸‹ä¸€çŠ¶æ€-åŠ¨ä½œå¯¹çš„æŠ˜æ‰£ Q å€¼
            u_t += (self.gamma * self.get_weights(next_observation, next_action))

        # å‡å°å½“å‰è¿¹çº¿çš„å¼ºåº¦ (é€’å‡å› å­ç”± gamma å’Œ Î» å…±åŒå†³å®š)
        self.e_tracy *= (self.gamma * self.lamda)

        # æ ¹æ®å½“å‰çŠ¶æ€å’ŒåŠ¨ä½œè·å–å¯¹åº”ç‰¹å¾ç´¢å¼•
        features = self.process_encode(observation, action)

        # å°†å½“å‰çŠ¶æ€-åŠ¨ä½œå¯¹åº”çš„ç‰¹å¾ç´¢å¼•çš„è¿¹çº¿å€¼è®¾ç½®ä¸º 1
        # è¿™è¡¨ç¤ºæœ€è¿‘è®¿é—®çš„çŠ¶æ€-åŠ¨ä½œå¯¹æœ‰æœ€é«˜çš„æ›´æ–°ä¼˜å…ˆçº§
        self.e_tracy[features] = 1.

        # è®¡ç®— TD è¯¯å·® (Temporal Difference Error)
        td_error = u_t - self.get_weights(observation, action)

        # æ ¹æ® TD è¯¯å·®ä»¥åŠè¿¹çº¿å€¼æ›´æ–°æ‰€æœ‰æƒé‡
        # è¿¹çº¿å€¼è¡¨ç¤ºå†å²ä¸ŠçŠ¶æ€-åŠ¨ä½œå¯¹å¯¹å½“å‰å­¦ä¹ è¿‡ç¨‹çš„å½±å“ç¨‹åº¦
        self.weights += (self.learning_rate * td_error * self.e_tracy)

        # å¦‚æœæ˜¯ç»ˆæ­¢çŠ¶æ€ï¼Œå°†è¿¹çº¿å€¼é‡ç½®ä¸ºé›¶
        if done:
            self.e_tracy = np.zeros_like(self.e_tracy)

    def play_game_by_sarsa_lamda(self, train=False):
        """
        ä½¿ç”¨SARSAç®—æ³•è®­ç»ƒ
        :param train:
        :return:
        """
        episode_reward = 0
        observation, _ = self.reset()  # observationï¼š[ä½ç½®ï¼Œ é€Ÿåº¦]
        action = self.agent_resemble_decide(observation)
        done = False
        while True:
            if self.render:
                self.env.render()

            next_observation, reward, terminated, truncated, _ = self.step(action)

            if not train:
                logger.info(f"ä¸‹ä¸€ä¸ªçŠ¶æ€ï¼š{next_observation}")
            episode_reward += reward

            next_action = self.agent_resemble_decide(next_observation)

            if not train:
                logger.info(f"ä¸‹ä¸€ä¸ªåŠ¨ä½œï¼š{self.translate_action[action]}")

            if terminated or truncated:
                done = True

            if train:
                self.sarsa_lamda_learn(observation, action, reward, next_observation, next_action, done)
            else:
                time.sleep(2)

            if done:
                logger.info(f"ç»“æŸä¸€è½®æ¸¸æˆ")
                break
            observation, action = next_observation, next_action
        return episode_reward


class DQNReplayer:
    """
    ç»éªŒå›æ”¾ç±»ï¼Œç”¨äºå­˜å‚¨å’Œé‡‡æ · DQN ä¸­çš„ç»éªŒã€‚

    ç»éªŒå›æ”¾æ± ï¼ˆReplay Bufferï¼‰ç”¨äºå­˜å‚¨æ™ºèƒ½ä½“åœ¨ä¸ç¯å¢ƒäº¤äº’è¿‡ç¨‹ä¸­ç”Ÿæˆçš„ç»éªŒï¼ˆçŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ã€ä¸‹ä¸€ä¸ªçŠ¶æ€ã€æ˜¯å¦ç»ˆæ­¢ï¼‰ã€‚è¿™äº›ç»éªŒéšåç”¨äºè®­ç»ƒç¥ç»ç½‘ç»œï¼Œä»¥ä½¿æ¨¡å‹å­¦ä¹ åˆ°æœ€ä½³ç­–ç•¥ã€‚
    """

    def __init__(self, capacity):
        """
        åˆå§‹åŒ–ç»éªŒå›æ”¾æ± ã€‚

        å‚æ•°ï¼š
        - capacity: intï¼Œå›æ”¾æ± çš„å®¹é‡ï¼Œå†³å®šäº†æœ€å¤šèƒ½å­˜å‚¨å¤šå°‘æ¡ç»éªŒã€‚
        """
        self.memory = pd.DataFrame(index=range(capacity),
                                   columns=['observation',  # å½“å‰çŠ¶æ€
                                            'action',  # æ‰§è¡ŒåŠ¨ä½œ
                                            'reward',  # æ”¶åˆ°çš„å¥–åŠ±
                                            'next_observation',  # ä¸‹ä¸€çŠ¶æ€
                                            'done'])  # æ˜¯å¦ç»ˆæ­¢æ ‡å¿—
        self.index = 0  # å½“å‰å­˜å‚¨ä½ç½®çš„ç´¢å¼•
        self.count = 0  # å½“å‰å›æ”¾æ± ä¸­å­˜å‚¨çš„ç»éªŒæ¡æ•°
        self.capacity = capacity  # å›æ”¾æ± çš„æœ€å¤§å®¹é‡

    def replay_store(self, *args, pbar=None):
        """
        å°†æ–°çš„ç»éªŒå­˜å‚¨åˆ°å›æ”¾æ± ã€‚

        å‚æ•°ï¼š
        - args: åŒ…å«ä¸€æ¡ç»éªŒçš„äº”ä¸ªå…ƒç´ ï¼ˆå½“å‰çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ã€ä¸‹ä¸€ä¸ªçŠ¶æ€ã€æ˜¯å¦ç»ˆæ­¢ï¼‰
        """
        # å­˜å‚¨ç»éªŒ
        self.memory.loc[self.index] = args
        # æ›´æ–°å­˜å‚¨ä½ç½®çš„ç´¢å¼•
        self.index = (self.index + 1) % self.capacity
        # å¢åŠ å›æ”¾æ± ä¸­å­˜å‚¨çš„ç»éªŒæ¡æ•°ï¼Œå¹¶ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§å®¹é‡
        self.count = min(self.count + 1, self.capacity)

        # å¦‚æœè¿›åº¦æ¡å­˜åœ¨ï¼Œæ›´æ–°è¿›åº¦æ¡
        if pbar is not None:
            pbar.update(1)

    def replay_sample(self, size):
        """
        ä»ç»éªŒå›æ”¾æ± ä¸­éšæœºé‡‡æ ·ä¸€æ‰¹ç»éªŒã€‚

        å‚æ•°ï¼š
        - size: intï¼Œè¦é‡‡æ ·çš„ç»éªŒæ•°é‡

        """
        # ä»å­˜å‚¨çš„ç»éªŒä¸­éšæœºé€‰æ‹©ç´¢å¼•
        indices = np.random.choice(self.count, size=size)
        return (np.stack(self.memory.loc[indices, field]) for field in self.memory.columns)


from torch.utils.tensorboard import SummaryWriter


class DQNAgentTorch(EnvInit):
    """
    Deep Q-learning Network with PyTorch
    """

    def __init__(self,
                 gamma: float = 0.99,
                 epsilon: float = 0.1,
                 replayer_capacity: int = 10000,
                 batch_size: int = 64):
        super().__init__()
        # è¶…å‚æ•°è®¾ç½®
        net_kwargs = {'hidden_sizes': [64, ]}  # ç¥ç»ç½‘ç»œéšè—å±‚è®¾ç½®
        self.Action_Num = self.env.action_space.n  # åŠ¨ä½œç©ºé—´çš„ç»´åº¦
        observation_dim = self.env.observation_space.shape[0]  # çŠ¶æ€ç©ºé—´ç»´åº¦
        self.gamma = gamma  # æŠ˜æ‰£å› å­
        self.epsilon = epsilon  # æ¢ç´¢æ¦‚ç‡

        # TensorBoard writer ç”¨äºè®°å½•è®­ç»ƒæ—¥å¿—
        current_time = time.localtime()
        log_dir = time.strftime("runs/dqn_torch/%Y_%m_%d_%H_%M", current_time)
        self.writer = SummaryWriter(log_dir=log_dir)

        # å…¶ä»–è¶…å‚æ•°
        self.learn_step_counter = int(0)  # å­¦ä¹ æ­¥è®¡æ•°å™¨
        self.learning_rate = 0.001 # å­¦ä¹ ç‡
        self.goal_position = 0.5
        self.batch_size = batch_size # # è¡¨ç¤ºæ¯æ¬¡è®­ç»ƒä»æ•°æ®é›†ä¸­æå– batch_size ä¸ªæ ·æœ¬
        self.replay_start_size = 1000  # ç»éªŒæ± å¼€å§‹è®­ç»ƒæ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°é‡
        self.update_lr_steps = 10000 # å­¦ä¹ ç‡åˆ·æ–°é—´éš”

        # ç”¨äºè·Ÿè¸ªæœ€è¿‘æ¸¸æˆçš„å®Œæˆç‡
        self.done_rate = deque(maxlen=100)

        # åˆå§‹åŒ–ç»éªŒæ± 
        self.replayer = DQNReplayer(replayer_capacity)

        # åˆå§‹åŒ–è¯„ä¼°ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.evaluate_net_pytorch = self.build_torch_network(input_size=observation_dim,
                                                             output_size=self.Action_Num, **net_kwargs)
        self.target_net_pytorch = self.build_torch_network(input_size=observation_dim,
                                                           output_size=self.Action_Num, **net_kwargs)

        # ä¼˜åŒ–å™¨
        self.dqn_optimizer = torch.optim.Adam(self.evaluate_net_pytorch.parameters(), lr=self.learning_rate)

        # å¦‚æœåŠ è½½æ¨¡å‹
        if self.load_model:
            checkpoint = torch.load("tools/policy_dir/MountainCar/evaluate_net_pytorch.pth", weights_only=True)
            self.evaluate_net_pytorch.load_state_dict(checkpoint["model_state_dict"])
            self.dqn_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            logger.info(f"æˆåŠŸåŠ è½½--->evaluate_net_pytorch")

        # å°†è¯„ä¼°ç½‘ç»œçš„æƒé‡å¤åˆ¶åˆ°ç›®æ ‡ç½‘ç»œ
        self.target_net_pytorch.load_state_dict(self.evaluate_net_pytorch.state_dict())

    def build_torch_network(self,
                            input_size,
                            hidden_sizes,
                            output_size,
                            activation=nn.ReLU,
                            output_activation=None):
        """
        æ„å»ºç®€å•çš„å‰é¦ˆç¥ç»ç½‘ç»œ
        """
        layers = []
        input_dim = input_size

        for hidden_size in hidden_sizes:
            layers.append(nn.Linear(input_dim, hidden_size))  # å…¨è¿æ¥å±‚
            layers.append(activation())  # æ¿€æ´»å‡½æ•°
            input_dim = hidden_size

        layers.append(nn.Linear(input_dim, output_size))  # è¾“å‡ºå±‚

        if output_activation:
            layers.append(output_activation())

        model = nn.Sequential(*layers)  # é¡ºåºæ¨¡å‹
        return model

    def dqn_torch_agent_learn(self, observation, action, reward, next_observation, done):
        """
        ä½¿ç”¨ DQN ç®—æ³•æ›´æ–°ç½‘ç»œ
        """
        self.evaluate_net_pytorch.train()  # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼

        # å¦‚æœç»éªŒæ± æ ·æœ¬ä¸è¶³ï¼Œè¿›è¡ŒåŠ è½½
        if self.replayer.count <= self.replay_start_size:
            with tqdm(total=10000, initial=self.replayer.count, dynamic_ncols=True, desc="ç»éªŒæ± åŠ è½½è¿›åº¦") as pbar:
                for _ in range(10000):
                    self.replayer.replay_store(observation, action, reward, next_observation, done, pbar=pbar)
                    time.sleep(0.0002)  # æ¨¡æ‹ŸåŠ è½½å»¶è¿Ÿ

        # å­˜å‚¨ç»éªŒå¹¶é‡‡æ ·
        self.replayer.replay_store(observation, action, reward, next_observation, done)
        observations, actions, rewards, next_observations, dones = self.replayer.replay_sample(self.batch_size)

        # è½¬æ¢ä¸º PyTorch å¼ é‡
        observations = torch.tensor(observations, dtype=torch.float32)
        actions = torch.tensor(actions, dtype=torch.long)
        rewards = torch.tensor(rewards, dtype=torch.float32)
        next_observations = torch.tensor(next_observations, dtype=torch.float32)
        dones = torch.tensor(dones, dtype=torch.float32)

        # ç›®æ ‡ç½‘ç»œè®¡ç®— Q å€¼
        next_qs = self.target_net_pytorch(next_observations).detach()
        next_max_qs = next_qs.max(dim=-1)[0]
        us = rewards + self.gamma * next_max_qs * (1. - dones)

        # å½“å‰ Q å€¼è®¡ç®—
        qs = self.evaluate_net_pytorch(observations)
        targets = qs.clone()
        targets[torch.arange(self.batch_size), actions] = us

        # æŸå¤±å‡½æ•°è®¡ç®—
        loss = nn.SmoothL1Loss()(qs, targets)

        # è®°å½•æŸå¤±å’Œå¹³å‡ Q å€¼
        if self.learn_step_counter % 50 == 0:
            self.writer.add_scalar("Loss/train", loss.item(), self.learn_step_counter)
            avg_q_value = qs.mean().item()
            self.writer.add_scalar("Q Value/Average", avg_q_value, self.learn_step_counter)

        # åå‘ä¼ æ’­æ›´æ–°æƒé‡
        self.evaluate_net_pytorch.zero_grad()
        loss.backward()
        self.dqn_optimizer.step()

    def dqn_torch_agent_decide(self, observation):
        """
        æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©åŠ¨ä½œ
        """
        if np.random.rand() < self.epsilon:  # è¿›è¡Œéšæœºæ¢ç´¢
            return np.random.randint(self.Action_Num)

        observation = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)
        qs = self.evaluate_net_pytorch(observation)
        return qs.argmax(dim=1).item()

    def play_game_by_dqn_torch_learning(self, train=False):
        """
        ä½¿ç”¨ DQN ç®—æ³•è®­ç»ƒå’Œè¯„ä¼°
        :param train: æ˜¯å¦è®­ç»ƒæ¨¡å¼
        :return: ç´¯ç§¯å¥–åŠ±
        """
        episode_reward = 0
        observation, _ = self.reset()
        done = False

        if not train:
            logger.info(f"****å¯åŠ¨è¯„ä¼°é˜¶æ®µ****")
            self.evaluate_net_pytorch.eval()
            self.target_net_pytorch.eval()

        while True:
            if self.render:
                self.env.render()

            if not train:
                with torch.no_grad():
                    action = self.dqn_torch_agent_decide(observation)
            else:
                action = self.dqn_torch_agent_decide(observation)

            next_observation, reward, terminated, truncated, _ = self.step(action)
            episode_reward += reward

            if terminated or truncated:
                done = True
                self.learn_step_counter += 1

            if train:
                self.dqn_torch_agent_learn(observation, action, reward, next_observation, done)

            if done:
                logger.info(f"ç»“æŸä¸€è½®æ¸¸æˆ")
                flag = True if episode_reward > -200 else False
                self.done_rate.append(flag)
                if train:
                    if self.learn_step_counter % 2000 == 0:
                        self.epsilon = max(0.01, self.epsilon * 0.995)
                    if self.learn_step_counter and self.learn_step_counter % 100 == 0:
                        self.target_net_pytorch.load_state_dict(self.evaluate_net_pytorch.state_dict())
                break

            observation = next_observation
        return episode_reward

    def refresh_writer(self, step):
        """
        åˆ·æ–° TensorBoard Writer
        """
        self.writer.close()
        current_time = time.localtime()
        log_dir = time.strftime("runs/dqn_torch/%Y_%m_%d_%H_%M", current_time)
        new_log_dir = f"{log_dir}/{step}"
        self.writer = SummaryWriter(log_dir=new_log_dir)

    def close(self):
        """
        å…³é—­ TensorBoard SummaryWriter
        """
        self.writer.close()
        logger.info("TensorBoard SummaryWriter å·²å…³é—­")


class DoubleDQNAgent(EnvInit):

    def __init__(self,
                 gamma: float = 0.99,
                 epsilon: float = 0.01,
                 replayer_capacity: int = 10000,
                 batch_size: int = 64):
        super().__init__()
        net_kwargs = {'hidden_sizes': [64, ]}
        self.Action_Num = self.env.action_space.n
        observation_dim = self.env.observation_space.shape[0]
        self.gamma = gamma
        self.epsilon = epsilon  #
        # TensorBoard writer
        current_time = time.localtime()
        log_dir = time.strftime("runs/double_dqn_torch/%Y_%m_%d_%H_%M", current_time)
        self.ddqn_writer = SummaryWriter(log_dir=log_dir)
        self.ddqn_learn_step_counter = int(0)  # å­¦ä¹ æ­¥æ•°è®¡æ•°å™¨
        self.ddqn_learning_rate = 0.0001
        self.ddqn_batch_size = batch_size
        self.ddqn_replay_start_size = 1000
        self.ddqn_training_started = False
        self.done_rate = deque(maxlen=100)
        self.done_rate.clear()
        self.ddqn_replayer = DQNReplayer(replayer_capacity)
        self.ddqn_evaluate_net_pytorch = self.ddqn_build_torch_network(input_size=observation_dim,
                                                                       output_size=self.Action_Num, **net_kwargs)
        self.ddqn_target_net_pytorch = self.ddqn_build_torch_network(input_size=observation_dim,
                                                                     output_size=self.Action_Num, **net_kwargs)
        self.ddqn_optimizer = torch.optim.Adam(self.ddqn_evaluate_net_pytorch.parameters(), lr=self.ddqn_learning_rate)

        if self.load_model:
            # åŠ è½½å‰ä¿å­˜æ¨¡å‹å‚æ•°
            # logger.info(f"initial_state_dict:{initial_state_dict}")
            checkpoint = torch.load("tools/policy_dir/MountainCar/ddqn_evaluate_net_pytorch.pth", weights_only=True)
            self.ddqn_evaluate_net_pytorch.load_state_dict(checkpoint["model_state_dict"])

            # logger.info(f"evaluate_net_pytorch:{self.evaluate_net_pytorch.state_dict()}")
            self.ddqn_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            # logger.info(f"dqn_optimizer:{self.dqn_optimizer}")
            logger.info(f"æˆåŠŸåŠ è½½--->evaluate_net_pytorch")

        self.ddqn_target_net_pytorch.load_state_dict(self.ddqn_evaluate_net_pytorch.state_dict())

    def ddqn_build_torch_network(self,
                                 input_size,
                                 hidden_sizes,
                                 output_size,
                                 activation=nn.ReLU,
                                 output_activation=None):
        """
        Build a simple feed-forward neural network with PyTorch
        """
        layers = []
        input_dim = input_size

        for hidden_size in hidden_sizes:
            layers.append(nn.Linear(input_dim, hidden_size))
            layers.append(activation())
            input_dim = hidden_size

        layers.append(nn.Linear(input_dim, output_size))

        if output_activation:
            layers.append(output_activation())

        model = nn.Sequential(*layers)
        return model

    def ddqn_torch_agent_decide(self, observation):
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.Action_Num)

        observation = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)
        qs = self.ddqn_evaluate_net_pytorch(observation)
        return qs.argmax(dim=1).item()

    def double_dqn_agent_learn(self, observation, action, reward, next_observation, done):
        self.ddqn_evaluate_net_pytorch.train()  # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼

        # åˆå§‹åŒ–è¿›åº¦æ¡
        if self.ddqn_replayer.count <= self.ddqn_replay_start_size:
            # logger.info(self.replayer.index)
            with tqdm(total=10000, initial=self.ddqn_replayer.count, dynamic_ncols=True, desc="ç»éªŒæ± åŠ è½½è¿›åº¦") as pbar:
                # å‡è®¾æˆ‘ä»¬ä¸æ–­å­˜å‚¨ç»éªŒ
                for _ in range(10000):
                    # å­˜å‚¨ç»éªŒå¹¶æ›´æ–°è¿›åº¦æ¡
                    self.ddqn_replayer.replay_store(observation, action, reward, next_observation, done, pbar=pbar)
                    time.sleep(0.0002)

        self.ddqn_replayer.replay_store(observation, action, reward, next_observation, done)
        observations, actions, rewards, next_observations, dones = self.ddqn_replayer.replay_sample(
            self.ddqn_batch_size)

        # Convert numpy arrays to PyTorch tensors
        observations = torch.tensor(observations, dtype=torch.float32)
        actions = torch.tensor(actions, dtype=torch.long)
        rewards = torch.tensor(rewards, dtype=torch.float32)
        next_observations = torch.tensor(next_observations, dtype=torch.float32)
        dones = torch.tensor(dones, dtype=torch.float32)

        # 1. è®¡ç®—å½“å‰ç½‘ç»œï¼ˆè¯„ä¼°ç½‘ç»œï¼‰åœ¨ next_observations ä¸Šçš„ Q å€¼
        next_eval_qs = self.ddqn_evaluate_net_pytorch(next_observations)

        # 2. è·å– next_eval_qs ä¸­çš„æœ€å¤§ Q å€¼çš„ç´¢å¼•ä½œä¸ºé€‰å®šçš„åŠ¨ä½œ
        next_actions = next_eval_qs.argmax(dim=-1)  # `argmax` ç”¨äºæ²¿ç€æŒ‡å®šçš„ç»´åº¦æ‰¾åˆ°æœ€å¤§å€¼çš„ç´¢å¼•

        # 3. è®¡ç®—ç›®æ ‡ç½‘ç»œï¼ˆtarget_netï¼‰åœ¨ next_observations ä¸Šçš„ Q å€¼
        next_qs = self.ddqn_target_net_pytorch(next_observations).detach()  # ä½¿ç”¨ç›®æ ‡ç½‘ç»œå¹¶ä¸” `detach()` é˜²æ­¢æ¢¯åº¦å›ä¼ 

        # 4. è·å–ç›®æ ‡ç½‘ç»œè¾“å‡ºçš„æ¯ä¸ªæ ·æœ¬çš„æœ€å¤§ Q å€¼ï¼ˆç”¨äºè®¡ç®— Q-learning çš„ç›®æ ‡å€¼ï¼‰
        next_max_qs = next_qs.gather(dim=-1, index=next_actions.unsqueeze(-1))  # gather æå–æ¯ä¸ªæ ·æœ¬å¯¹åº”çš„æœ€å¤§ Q å€¼
        next_max_qs = next_max_qs.squeeze(-1)  # ç§»é™¤æœ€åçš„ç»´åº¦ï¼Œä½¿å…¶ä¿æŒæ­£ç¡®çš„å½¢çŠ¶

        # Q values from target network
        # next_qs = self.target_net_pytorch(next_observations).detach()
        # next_max_qs = next_qs.max(dim=-1)[0]
        us = rewards + self.gamma * next_max_qs * (1. - dones)

        # Get current Q values
        qs = self.ddqn_evaluate_net_pytorch(observations)

        # Update the Q-values for the taken actions
        targets = qs.clone()
        targets[torch.arange(self.ddqn_batch_size), actions] = us

        # Compute loss
        # loss = nn.MSELoss()(qs, targets)
        loss = nn.SmoothL1Loss()(qs, targets)

        if self.ddqn_learn_step_counter % 50 == 0:  # æ¯ 50 æ­¥è®°å½•ä¸€æ¬¡
            self.ddqn_writer.add_scalar("Loss/train", loss.item(), self.ddqn_learn_step_counter)
            avg_q_value = qs.mean().item()
            self.ddqn_writer.add_scalar("Q Value/Average", avg_q_value, self.ddqn_learn_step_counter)

        # Back_propagate
        self.ddqn_evaluate_net_pytorch.zero_grad()
        loss.backward()
        # Update weights using optimizer
        self.ddqn_optimizer.step()

    def play_game_by_double_dqn_torch_learning(self, train=False):
        """
        ä½¿ç”¨Q-Learningç®—æ³•è®­ç»ƒ
        :param train: æ˜¯å¦æ˜¯è®­ç»ƒæ¨¡å¼
        :return: æŸä¸€è½®ç´¯ç§¯å¥–åŠ±
        """
        episode_reward = 0
        observation, _ = self.reset()
        done = False
        # åœ¨æ¨ç†é˜¶æ®µç¦ç”¨æ¢¯åº¦è®¡ç®—
        if not train:  # åªæœ‰åœ¨è¯„ä¼°é˜¶æ®µæ‰è¿›è¡Œæ¨ç†
            logger.info(f"****å¯åŠ¨è¯„ä¼°é˜¶æ®µ****")
            self.ddqn_evaluate_net_pytorch.eval()  # åˆ‡æ¢æ¨¡å‹åˆ°è¯„ä¼°æ¨¡å¼
            self.ddqn_target_net_pytorch.eval()  # åˆ‡æ¢ç›®æ ‡ç½‘ç»œåˆ°è¯„ä¼°æ¨¡å¼

        while True:
            if self.render:
                self.env.render()

            # åœ¨æ¨ç†é˜¶æ®µç¦ç”¨æ¢¯åº¦è®¡ç®—
            if not train:  # åªæœ‰åœ¨è¯„ä¼°é˜¶æ®µæ‰è¿›è¡Œæ¨ç†
                with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼Œé¿å…ä¸å¿…è¦çš„å†…å­˜ä½¿ç”¨
                    # start = time.time()
                    action = self.ddqn_torch_agent_decide(observation)  # æ¨ç†å†³ç­–
                    # end = time.time()
                    # logger.info(f"é€‰æ‹©-->{action}---{int(end-start)}ç§’")
            else:
                action = self.ddqn_torch_agent_decide(observation)

            next_observation, reward, terminated, truncated, _ = self.step(action)
            # if not train:  # åªæœ‰åœ¨è¯„ä¼°é˜¶æ®µæ‰è¿›è¡Œæ¨ç†
            # logger.info(f"çŠ¶æ€-->{next_observation}")
            # logger.info(f"å¥–åŠ±-->{reward}")

            episode_reward += reward

            if terminated or truncated:
                done = True
                self.ddqn_learn_step_counter += 1

            if train:
                self.double_dqn_agent_learn(observation, action, reward, next_observation, done)
            else:
                time.sleep(0)

            if done:
                logger.info(f"ç»“æŸä¸€è½®æ¸¸æˆ")
                flag = True if episode_reward > -200 else False
                self.done_rate.append(flag)
                if train:
                    # if self.ddqn_learn_step_counter % 2500 == 0:  # æ¯ 1000 æ­¥åˆ·æ–°ä¸€æ¬¡
                    #     self.ddqn_refresh_writer(self.ddqn_learn_step_counter)
                    if self.ddqn_learn_step_counter % 2000 == 0:  # æ¯ 1000 æ­¥åˆ·æ–°ä¸€æ¬¡
                        self.epsilon = max(0.01, self.epsilon * 0.995)  # æ¯æ¬¡å‡å°‘ï¼Œæœ€ä½ä¸º 0.01
                    if self.ddqn_learn_step_counter and self.ddqn_learn_step_counter % 100 == 0:
                        self.ddqn_target_net_pytorch.load_state_dict(self.ddqn_evaluate_net_pytorch.state_dict())
                break

            observation = next_observation
        return episode_reward

    def ddqn_refresh_writer(self, step):
        self.ddqn_writer.close()  # å…³é—­æ—§çš„ writer
        new_log_dir = f"runs/double_dqn_torch_{step}"
        self.ddqn_writer = SummaryWriter(log_dir=new_log_dir)


class MountainCar(SARSAAgent, SARSALamdaAgent, DQNAgentTorch, DoubleDQNAgent):
    def __init__(self):
        SARSAAgent.__init__(self)
        SARSALamdaAgent.__init__(self)
        DQNAgentTorch.__init__(self)
        DoubleDQNAgent.__init__(self)
        self.class_name = self.__class__.__name__

    def play_game(self):
        """
        æ™ºèƒ½ä½“æ¨æ¼”
        :return:
        """
        self.print_env_info()
        observation, _ = self.reset()
        while True:
            self.positions.append(observation[0])
            self.velocities.append(observation[1])
            next_observation, reward, terminated, truncated, _ = self.step(2)
            done = terminated or truncated
            if done:
                break
            observation = next_observation

        if next_observation[0] > 0.5:
            logger.info("æˆåŠŸ")
        else:
            logger.info("å¤±è´¥")

        Visualizer.plot_maintain_curve(self.positions, self.velocities)

    def game_iteration(self, show_policy):
        """
        è¿­ä»£
        :param show_policy: ä½¿ç”¨çš„æ›´æ–°ç­–ç•¥æ–¹å¼
        """
        episode_reward = 0.
        episode_rewards = []  # æ€»è½®æ•°çš„å¥–åŠ±(æŸè½®æ€»å¥–åŠ±)åˆ—è¡¨
        logger.info(f"*****å¯åŠ¨: {show_policy}*****")
        method_name = "default"
        is_train = False
        for game_round in range(1, self.game_rounds):
            logger.info(f"---ç¬¬{game_round}è½®è®­ç»ƒ---")

            if show_policy == "å‡½æ•°è¿‘ä¼¼SARSAç®—æ³•":
                # logger.info(f"å‡½æ•°è¿‘ä¼¼SARSAç®—æ³•")
                episode_reward = self.play_game_by_sarsa_resemble(train=True)  # ç¬¬roundè½®æ¬¡çš„ç´¯ç§¯reward
                method_name = self.play_game_by_sarsa_resemble.__name__
            if show_policy == "å‡½æ•°è¿‘ä¼¼SARSA(ğœ†)ç®—æ³•":
                # logger.info(f"å‡½æ•°è¿‘ä¼¼SARSA(ğœ†)ç®—æ³•")
                episode_reward = self.play_game_by_sarsa_lamda(train=True)  # ç¬¬roundè½®æ¬¡çš„ç´¯ç§¯reward
                method_name = self.play_game_by_sarsa_lamda.__name__
            if show_policy == "æ·±åº¦Qå­¦ä¹ ç®—æ³•_pytorch":
                # logger.info(f"å¯åŠ¨ï¼šæ·±åº¦Qå­¦ä¹ ç®—æ³•_pytorchç®—æ³•")
                if game_round > 0 and game_round % self.update_lr_steps == 0:
                    self.learning_rate *= 0.1
                    logger.info(f"æ›´æ–°å­¦ä¹ ç‡:: {self.learning_rate},ä¸‹é™0.1")
                episode_reward = self.play_game_by_dqn_torch_learning(train=False)  # ç¬¬roundè½®æ¬¡çš„ç´¯ç§¯reward
                method_name = self.play_game_by_dqn_torch_learning.__name__
            if show_policy == "Doubleæ·±åº¦Qå­¦ä¹ ç®—æ³•_pytorch":
                # logger.info(f"å¯åŠ¨ï¼šæ·±åº¦Qå­¦ä¹ ç®—æ³•_pytorchç®—æ³•")
                if game_round > 0 and game_round % self.update_lr_steps == 0:
                    self.ddqn_learning_rate *= 0.1
                    logger.info(f"æ›´æ–°å­¦ä¹ ç‡:: {self.ddqn_learning_rate},ä¸‹é™0.1")
                episode_reward = self.play_game_by_double_dqn_torch_learning(train=False)  # ç¬¬roundè½®æ¬¡çš„ç´¯ç§¯reward
                method_name = self.play_game_by_double_dqn_torch_learning.__name__

            if is_train and self.save_policy and (game_round % 150 == 0 or game_round == self.game_rounds - 1):
                if show_policy == "å‡½æ•°è¿‘ä¼¼SARSAç®—æ³•" or show_policy == "å‡½æ•°è¿‘ä¼¼SARSA(ğœ†)ç®—æ³•":
                    save_data = {
                        "weights": self.weights,
                        "encoder": self.tile_coder.codebook if self.tile_coder else None
                    }
                    Policy_loader.save_policy(method_name, self.class_name, save_data, step=game_round)
                if show_policy == "æ·±åº¦Qå­¦ä¹ ç®—æ³•_pytorch":
                    save_data = {"evaluate_net_pytorch": self.evaluate_net_pytorch,
                                 "target_net_pytorch": self.target_net_pytorch,
                                 "optimizer": self.dqn_optimizer}

                    Policy_loader.save_policy(method_name, self.class_name, save_data, step=game_round)
                if show_policy == "Doubleæ·±åº¦Qå­¦ä¹ ç®—æ³•_pytorch":
                    save_data = {"ddqn_evaluate_net_pytorch": self.ddqn_evaluate_net_pytorch,
                                 "ddqn_target_net_pytorch": self.ddqn_target_net_pytorch,
                                 "ddqn_optimizer": self.ddqn_optimizer}

                    Policy_loader.save_policy(method_name, self.class_name, save_data, step=game_round)

            if episode_reward is not None:
                episode_rewards.append(episode_reward)
                if is_train:
                    if self.learn_step_counter % 10 == 0:  # æ¯ 10 è½®è®°å½•ä¸€æ¬¡å¥–åŠ±
                        self.writer.add_scalar("Episode Reward", episode_reward, global_step=self.learn_step_counter)
                        self.ddqn_writer.add_scalar("Episode Reward", episode_reward,
                                                    global_step=self.ddqn_learn_step_counter)
                rate_every_length = (round((self.done_rate.count(True) / len(self.done_rate)), 2) * 100)
                logger.info(f"ï½œç¬¬{game_round}è½®å¥–åŠ±: ${episode_reward}"
                            f"ï½œ>>>>>>>"
                            f"ï½œå‰{len(self.done_rate)}å›åˆæˆåŠŸç‡:{rate_every_length}%ï½œ")
                if len(self.done_rate) == 100 and rate_every_length >= 80 and is_train:
                    logger.info(f"!!!æˆåŠŸç‡å·²ç»è¾¾åˆ°70%ï¼Œè‡ªåŠ¨åœæ­¢è®­ç»ƒ!!!")
                    break


            else:
                logger.warning(f"ç¬¬{game_round}è½®å¥–åŠ±ä¸º Noneï¼Œå·²è·³è¿‡ã€‚")

            Visualizer.plot_cumulative_avg_rewards(episode_rewards, game_round, self.game_rounds, self.class_name,
                                                   method_name)

        print(
            f"å¹³å‡å¥–åŠ±ï¼š{(np.round(np.mean(episode_rewards), 2))} = {np.sum(episode_rewards)} / {len(episode_rewards)}")
        print(
            f"æœ€å100è½®å¥–åŠ±ï¼š{(np.round(np.mean(episode_rewards[-500:]), 2))} = {np.sum(episode_rewards[-500:])} / {len(episode_rewards[-500:])}")
        logger.info(f"*****ç»“æŸ: {show_policy}*****")
