# -*- coding: utf-8 -*-
"""
@File    : mountaincar.py      # æ–‡ä»¶åï¼Œmountaincarè¡¨ç¤ºå½“å‰æ–‡ä»¶å
@Time    : 2025/1/10         # åˆ›å»ºæ—¶é—´ï¼Œ2025/1/10è¡¨ç¤ºå½“å‰æ—¶é—´
@Author  : <your_name>     # ä½œè€…
@Email   : <your_email>    # ä½œè€…ç”µå­é‚®ä»¶
@Desc    : <brief_description> # æ–‡ä»¶çš„ç®€è¦æè¿°
"""

import time

import gym
import numpy as np
import logging

from envs.env_template import Env
from tools.visualizer import Visualizer
from tools.save_policy import Policy_loader

logger = logging.getLogger(__name__)  # ä½¿ç”¨å½“å‰æ¨¡å—å
from envs.global_set import *


class EnvInit(Env):
    """
    ç®—æ³•å‚æ•°åˆå§‹åŒ–
    """

    def __init__(self, name='MountainCar-v0', render_mode=render_model[0], render=False):
        super().__init__(name, render_mode, render)
        # æ˜¯å¦å¼€å¯åŠ¨ç”»
        if render:
            self.env = gym.make(name, render_mode=render_mode)
        else:
            self.env = gym.make(name)

        self.render = render
        # æ¸¸æˆè½®æ•°
        self.game_rounds = 300
        # self.State_Num = self.env.observation_space.n
        # è·å–åŠ¨ä½œç©ºé—´çš„å¤§å°ï¼Œå³å¯é€‰æ‹©çš„åŠ¨ä½œæ•°é‡
        self.Action_Num = self.env.action_space.n
        # ä½ç½®
        self.positions = []
        # é€Ÿåº¦
        self.velocities = []
        # ä¿å­˜æ¨¡å‹
        self.save_policy = True
        # åŠ è½½æ¨¡å‹
        self.load_model = True
        self.train = True
        # æŠ˜æ‰£å› å­ï¼Œå†³å®šäº†æœªæ¥å¥–åŠ±çš„å½±å“
        self.gamma = 1.
        # å­¦ä¹ ç‡
        self.learning_rate = 0.03
        # æŸ¯è¥¿æ”¶æ•›èŒƒå›´
        self.tolerant = 1e-6
        # Îµ-æŸ”æ€§ç­–ç•¥å› å­
        self.epsilon = 0.001

    @property
    def print_env_info(self):
        return self.__env_info

    def __env_info(self):
        logger.info(f'è§‚æµ‹ç©ºé—´ï¼š{self.envs.observation_space}')
        logger.info(f'åŠ¨ä½œç©ºé—´ï¼š{self.envs.action_space}')
        logger.info(f'ä½ç½®èŒƒå›´ï¼š{(self.envs.min_position, self.envs.max_position)}')
        logger.info(f'é€Ÿåº¦èŒƒå›´ï¼š{(-self.envs.max_speed, self.envs.max_speed)}')
        logger.info(f'ç›®æ ‡ä½ç½®ï¼š{self.envs.goal_position}')


class TileCoder:
    """
    ç“¦ç –ç¼–ç ï¼šå¯¹äºå¯ä»¥å®ç°ç›®æ ‡å‡½æ•°çš„çŠ¶æ€ï¼Œå°½å¯èƒ½æ•æ‰ç›¸ä¼¼æ€§ï¼Œå¯¹äºæ— æ³•å®Œæˆç›®æ ‡å‡½æ•°çš„çŠ¶æ€ï¼Œå°½å¯èƒ½åŒºåˆ†å·®å¼‚æ€§
    è¾“å…¥ç‰¹å¾å¯èƒ½æ˜¯ï¼š[ä½ç½®ï¼Œéšœç¢ç‰©ï¼Œç›®æ ‡è·ç¦»ï¼Œæ–¹å‘ä¿¡æ¯]ï¼Œå¯¹åº”çš„æƒé‡ä¹Ÿæœ‰ç›¸åŒç»´åº¦ï¼Œ
    ä½†æ˜¯åœ¨å…·ä½“åˆ†ç»„æ—¶ï¼Œä¼šæ ¹æ®å…·ä½“æƒ…å†µåˆ†é…æƒé‡ï¼Œæ¯”å¦‚éšœç¢ç‰©ï¼Œï¼ˆ1ï¼Œ-10ï¼Œ10ï¼Œ5ï¼‰ï¼Œå°†æ•´ä½“ä»·å€¼æ‹‰ä½
    è€Œåœ¨çº¿æ€§å‡½æ•°ä¸­ä¼šæ ¹æ®çŠ¶æ€çš„æ€»ä½“ä¸åŒæƒ…å†µåˆ†ç»„ï¼Œæ¯”å¦‚é¦–å…ˆæ˜¯éšœç¢ç‰©ï¼Œç„¶åæ˜¯ç›®æ ‡è·ç¦»...ï¼Œä¸ä¼šå¯¹æ‰€æœ‰æƒ…å†µè®¡ç®—æƒé‡ï¼Œ
    è¿™å°±å®ç°äº†æ³›åŒ–ï¼Œå³ç›¸ä¼¼ç‰¹å¾å¯ä»¥ä½¿ç”¨ç›¸åŒæƒé‡

    æœ¬è´¨ï¼šç¼–ç è¿‡ç¨‹å°±æ˜¯å¯¹çœŸå®ä¸–ç•Œç‰©ç†é‡ä¾¿äºä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒè€Œå°†è¿ç»­çš„çŠ¶æ€è½¬åŒ–ä¸ºç¦»æ•£çš„è¡¨ç¤ºï¼Œ
    åœ¨å®šä¹‰åï¼Œæ‰€æœ‰è®­ç»ƒçš„çŠ¶æ€å‘é‡éƒ½åº”è¯¥éµå¾ªè¿™ä¸ªè§„åˆ™ï¼Œåœ¨è®­ç»ƒåï¼Œåœ¨å°†è¾“å‡ºä¼ é€’ç»™ç°å®ä¸–ç•Œè¿›è¡Œå†³ç­–è§„åˆ’ï¼Œ

    ä¼ æ„Ÿå™¨é‡‡æ ·å¾—åˆ°çš„ç‰©ç†ä¿¡æ¯ï¼ˆä¾‹å¦‚ä½ç½®ã€é€Ÿåº¦ã€è§’åº¦ç­‰ï¼‰ä¼šå®æ—¶ä¼ é€’ç»™ç“¦ç –ç½‘ç»œè¿›è¡Œç¼–ç ï¼Œ
    ç“¦ç –ç½‘ç»œè´Ÿè´£å°†è¿™äº›è¿ç»­çš„ç‰©ç†ä¿¡æ¯ç¦»æ•£åŒ–ï¼Œ
    å¹¶é€šè¿‡ç‰¹å®šçš„ç¼–ç æ–¹å¼ï¼ˆä¾‹å¦‚ç“¦ç –ç¼–ç ã€å“ˆå¸Œç¼–ç ç­‰ï¼‰å°†è¿™äº›ä¿¡æ¯è½¬æ¢ä¸ºé€‚åˆç”¨äºè®­ç»ƒçš„ç‰¹å¾è¡¨ç¤º
    """

    def __init__(self, layers, features, codebook=None):
        self.layers = layers  # ç“¦ç –çš„å±‚æ•°
        self.features = features  # æœ€å¤šèƒ½å¤Ÿå­˜å‚¨çš„ç‰¹å¾æ•°ï¼Œæƒé‡å‚æ•°çš„ç»´åº¦
        self.codebook = codebook if codebook else {}  # ç”¨äºå­˜å‚¨æ¯ä¸ªç¼–ç å¯¹åº”çš„ç‰¹å¾

    @property
    def get_features(self):
        return self.__get_features

    def __get_features(self, codeword):
        # codebook = {(0, 25, 10, 1): 0, (0, 25, 10, 2): 1, (0, 25, 11, 1): 2}
        logger.debug(f"codeword:{codeword}")
        codeword = tuple(codeword)
        if codeword in self.codebook:
            return self.codebook[codeword]  # å¦‚æœå·²ç»è®¡ç®—è¿‡è¿™ä¸ªç¼–ç ï¼Œåˆ™è¿”å›å¯¹åº”çš„ç‰¹å¾ID
        # æ¯æ¬¡å¤šä¸ªcodewordï¼Œ+1
        count = len(self.codebook)
        if count >= self.features:
            return hash(codeword) % self.features  # å¦‚æœç‰¹å¾æ•°é‡è¶…å‡ºæœ€å¤§é™åˆ¶ï¼Œè¿›è¡Œå“ˆå¸Œæ˜ å°„ï¼Œ
            # è¯¥hashå°†é‡Œé¢çš„tupleå¤šä¸ªå€¼è®¡ç®—å‡ºä¸€ä¸ªæ•´æ•°ï¼Œå†å–æ¨¡é˜²æ­¢å“ˆå¸Œç¢°æ’
        else:
            self.codebook[codeword] = count  # å¦‚æœç‰¹å¾æ•°é‡æœªè¶…å‡ºé™åˆ¶ï¼Œåˆ™ä¸ºè¯¥ç¼–ç åˆ†é…ä¸€ä¸ªæ–°çš„ç‰¹å¾ID
            return count

    def __call__(self, floats=(), ints=()):
        """
        floats: æµ®åŠ¨ç‰¹å¾ï¼Œç¦»æ•£åŒ–çš„è¿ç»­è¾“å…¥ç‰¹å¾, floats = (3.4, 1.2)

        # åˆ›å»º BrickNetwork ç±»çš„å®ä¾‹ï¼Œå‡è®¾å±‚æ•°ä¸º 3
        network = BrickNetwork(layers=3)
        # è°ƒç”¨å®ä¾‹ï¼Œä¼ å…¥æµ®åŠ¨ç‰¹å¾ (ä½ç½®ã€é€Ÿåº¦) å’Œæ•´æ•°ç‰¹å¾ (ä¾‹å¦‚åŠ¨ä½œ)
        floats = (3.4, 1.2)  # å‡è®¾ä½ç½®æ˜¯ 3.4ï¼Œé€Ÿåº¦æ˜¯ 1.2
        ints = (0,)  # å‡è®¾æ•´æ•°ç‰¹å¾æ˜¯ 0ï¼Œå¯èƒ½ä»£è¡¨æŸä¸ªåŠ¨ä½œ
        # ä½¿ç”¨ __call__ æ–¹æ³•ï¼ˆå®é™…ä¸Šæ˜¯ç›´æ¥é€šè¿‡å®ä¾‹è°ƒç”¨ï¼‰å¾—åˆ°ç¦»æ•£åŒ–çš„ç‰¹å¾
        features = network(floats=floats, ints=ints)
        """
        dim = len(floats)

        # ä¸¾ä¾‹ï¼šå¯¹äºè¾“å…¥ä¸º(0,10)çš„åŒºé—´ï¼Œå¦‚æœè¢«layers=3åˆ’åˆ†ï¼Œä¸”æ¯ä¸ªåˆ’åˆ†çš„åç§»é‡ä¸åŒï¼Œ
        # ä¸åŒçš„ä½¿å¾—æ¯ä¸€å±‚çš„ç“¦ç –åˆ’åˆ†å…·æœ‰ä¸åŒçš„ç²¾åº¦å’Œè§†è§’ï¼Œå› æ­¤å¢å¼ºäº†ç¼–ç çš„è¡¨è¾¾èƒ½åŠ›ã€‚

        # ä¾‹å¦‚ï¼Œå‡è®¾å±‚æ•°
        # m = 3ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šå¯¹ä½ç½®ç‰¹å¾
        # xçš„æ¯ä¸€å±‚ä½¿ç”¨ä¸åŒçš„åç§»é‡ï¼š
        # ç¬¬ä¸€å±‚ï¼šä½ç½®xåˆ’åˆ†ä¸º[0, 3), [3, 6), [6, 9), [9, 10]
        # ç¬¬äºŒå±‚ï¼šä½ç½®xåˆ’åˆ†ä¸º[0, 2), [2, 5), [5, 8), [8, 10]
        # ç¬¬ä¸‰å±‚ï¼šä½ç½®xåˆ’åˆ†ä¸º[0, 1), [1, 4), [4, 7), [7, 10]
        # å¯ä»¥æŠŠç¼©æ”¾çœ‹ä½œæ˜¯é¢ç§¯çš„æ”¾å¤§ï¼Œå› ä¸ºé¢ç§¯æ˜¯x^2ï¼Œå½“xç¼©æ”¾3å€ï¼Œå°±æ˜¯3xï¼Œé¢ç§¯å°±æ˜¯3*3*x^2ï¼Œæ‰€ä»¥ï¼Œæ˜¯å¯¹äºæŸä¸€ä¸ªç‰¹å¾æ˜¯f*layer*layer
        scales_floats = tuple(f * self.layers * self.layers for f in floats)
        features = []
        for layer in range(self.layers):
            # 1 + dim * iç›®çš„æ˜¯ä¸ºäº†åœ¨ä¸åŒçš„å±‚ï¼ˆlayerï¼‰å’Œç‰¹å¾ï¼ˆiï¼‰ä¹‹é—´å¼•å…¥ä¸åŒçš„åç§»é‡ã€‚
            # å½“ i = 0 æ—¶ï¼Œåç§»é‡æ˜¯ 1 + 3 * 0 = 1ï¼Œè¿™å°±ç›¸å½“äºç»™ç¬¬ä¸€ä¸ªç‰¹å¾ï¼ˆæ¯”å¦‚ä½ç½®ï¼‰æ·»åŠ ä¸€ä¸ªåŸºæœ¬çš„åç§»é‡ 1ã€‚
            # å½“ i = 1 æ—¶ï¼Œåç§»é‡æ˜¯ 1 + 3 * 1 = 4ï¼Œè¿™å°±ç›¸å½“äºç»™ç¬¬äºŒä¸ªç‰¹å¾ï¼ˆæ¯”å¦‚é€Ÿåº¦ï¼‰æ·»åŠ ä¸€ä¸ªåç§»é‡ 4ã€‚
            # å½“ i = 2 æ—¶ï¼Œåç§»é‡æ˜¯ 1 + 3 * 2 = 7ï¼Œè¿™å°±ç›¸å½“äºç»™ç¬¬ä¸‰ä¸ªç‰¹å¾ï¼ˆæ¯”å¦‚è§’åº¦ï¼‰æ·»åŠ ä¸€ä¸ªåç§»é‡ 7ã€‚
            # å°†æ¯ä¸€å±‚çš„ç¦»æ•£åŒ–ç‰¹å¾å’Œæ•´æ•°ç‰¹å¾ï¼ˆå¦‚çŠ¶æ€æˆ–åŠ¨ä½œï¼‰ä¸€èµ·æ‹¼æ¥æˆä¸€ä¸ª codeword
            # dimä½œç”¨: å¢å¤§ä¸åŒç‰¹å¾ä¹‹é—´çš„åŒºåˆ«é˜²æ­¢ç‰¹å¾çš„åç§»é‡ç›¸äº’å¹²æ‰°ï¼›ç“¦ç –ç¼–ç çš„è¡¨è¾¾èƒ½åŠ›ä¸‹é™
            codeword = ((layer,) + tuple(int((f + (1 + dim * i) * layer) / self.layers)
                                         for i, f in enumerate(scales_floats)) +
                        (ints if isinstance(ints, tuple) else (ints,)))
            # codeword = (0, 25, 10, 1)
            feature = self.__get_features(codeword)
            features.append(feature)
        return features

class SARSAAgent(EnvInit):
    """
    å‡½æ•°è¿‘ä¼¼SARSAç®—æ³•
    1. ç»´åº¦æ•°é‡é—®é¢˜
    2. çŠ¶æ€ç¼–ç è¿‡ç¨‹
    3. è®­ç»ƒè¿‡ç¨‹ä»¥åŠç®—æ³•æ›´æ–°è¿‡ç¨‹
    """
    def __init__(self, layers=8, features=1893):
        """
        åˆå§‹åŒ–SARSA Agent
        :param layers: TileCoderçš„å±‚æ•°ï¼ˆå¤šå±‚ç¼–ç ç”¨äºæ›´ç»†ç²’åº¦çš„çŠ¶æ€è¡¨ç¤ºï¼‰
        :param features: æ€»çš„ç‰¹å¾æ•°é‡
        """
        super().__init__(render=True)  # åˆå§‹åŒ–çˆ¶ç±»ï¼ˆåŒ…å«ç¯å¢ƒç›¸å…³å‚æ•°ï¼‰
        self.render = False  # é»˜è®¤å…³é—­æ¸²æŸ“
        self.obs_low = self.env.observation_space.low  # ç¯å¢ƒè§‚æµ‹çš„æœ€å°å€¼
        self.obs_scale = self.env.observation_space.high - self.env.observation_space.low  # ç¯å¢ƒè§‚æµ‹çš„èŒƒå›´
        self.layers = layers  # TileCoder çš„å±‚æ•°
        self.features = features  # ç‰¹å¾æ•°é‡

        if not self.load_model:  # å¦‚æœæœªåŠ è½½æ¨¡å‹ï¼Œåˆ™åˆå§‹åŒ– TileCoder å’Œæƒé‡
            self.encoder = TileCoder(layers, features)  # åˆå§‹åŒ–TileCoderï¼Œç”¨äºçŠ¶æ€å’ŒåŠ¨ä½œçš„ç¼–ç 
            self.w = np.zeros(features)  # åˆå§‹åŒ–æƒé‡ä¸ºé›¶å‘é‡
        else:  # å¦‚æœåŠ è½½æ¨¡å‹ï¼Œåˆ™æ¢å¤æƒé‡å’Œç¼–ç å™¨çŠ¶æ€
            self.w, codebook = Policy_loader.load_w_para(class_name=self.__class__.__name__,
                                                         method_name="play_game_by_sarsa_resemble.pkl")
            self.encoder = TileCoder(layers, features, codebook)  # ä½¿ç”¨åŠ è½½çš„codebookåˆå§‹åŒ–TileCoder

    def encode(self, observation, action):
        """
        ç¼–ç è§‚æµ‹å’ŒåŠ¨ä½œä¸ºç‰¹å¾å‘é‡
        :param observation: å½“å‰çŠ¶æ€ï¼ˆè¿ç»­å€¼ï¼‰
        :param action: åŠ¨ä½œï¼ˆç¦»æ•£å€¼ï¼‰
        :return: ç‰¹å¾ç´¢å¼•åˆ—è¡¨
        """
        # å°†è§‚æµ‹å€¼å½’ä¸€åŒ–åˆ° [0, 1] èŒƒå›´ï¼Œå¹¶è½¬æ¢ä¸ºå…ƒç»„
        states = tuple((observation - self.obs_low) / self.obs_scale)
        # å°†åŠ¨ä½œå°è£…ä¸ºå…ƒç»„
        actions = (action,)
        # ä½¿ç”¨TileCoderç¼–ç ä¸ºç‰¹å¾ç´¢å¼•
        return self.encoder(states, actions)

    def get_q(self, observation, action):
        """
        è·å–åŠ¨ä½œçš„Qå€¼
        :param observation: å½“å‰çŠ¶æ€
        :param action: åŠ¨ä½œ
        :return: å¯¹åº”çš„Qå€¼
        """
        features = self.encode(observation, action)  # ç¼–ç è§‚æµ‹å’ŒåŠ¨ä½œä¸ºç‰¹å¾ç´¢å¼•
        return self.w[features].sum()  # æ ¹æ®æƒé‡å’Œç‰¹å¾è®¡ç®—Qå€¼

    def agent_resemble_decide(self, observation):
        """
        æ ¹æ®å½“å‰ç­–ç•¥è¿›è¡ŒåŠ¨ä½œå†³ç­–
        :param observation: å½“å‰çŠ¶æ€
        :return: é€‰å®šçš„åŠ¨ä½œ
        """
        if np.random.rand() < self.epsilon:  # ä»¥epsilonæ¦‚ç‡éšæœºé€‰æ‹©åŠ¨ä½œï¼ˆæ¢ç´¢ï¼‰
            return np.random.randint(self.Action_Num)
        else:  # å¦åˆ™é€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œï¼ˆåˆ©ç”¨ï¼‰
            qs = [self.get_q(observation, action) for action in range(self.Action_Num)]
            return np.argmax(qs)  # è¿”å›Qå€¼æœ€å¤§çš„åŠ¨ä½œç´¢å¼•

    def sarsa_resemble_learn(self, observation, action, reward, next_observation, next_action, done):
        """
        ä½¿ç”¨SARSAæ›´æ–°è§„åˆ™è¿›è¡Œå­¦ä¹ 
        :param observation: å½“å‰çŠ¶æ€
        :param action: å½“å‰åŠ¨ä½œ
        :param reward: å½“å‰å¥–åŠ±
        :param next_observation: ä¸‹ä¸€ä¸ªçŠ¶æ€
        :param next_action: ä¸‹ä¸€ä¸ªåŠ¨ä½œ
        :param done: æ˜¯å¦ä¸ºç»ˆæ­¢çŠ¶æ€
        """
        # è®¡ç®—ç›®æ ‡å€¼u
        u = reward + (1. - done) * self.gamma * self.get_q(next_observation, next_action)
        # TDè¯¯å·®ï¼šç›®æ ‡å€¼ä¸å½“å‰Qå€¼çš„å·®
        td_error = u - self.get_q(observation, action)
        # è·å–å½“å‰çŠ¶æ€å’ŒåŠ¨ä½œçš„ç‰¹å¾ç´¢å¼•
        features = self.encode(observation, action)
        # æ ¹æ®TDè¯¯å·®æ›´æ–°æƒé‡
        self.w[features] += self.learning_rate * td_error

    def play_game_by_sarsa_resemble(self, train=False):
        """
        ä½¿ç”¨SARSAç®—æ³•è®­ç»ƒ
        :param train:
        :return:
        """
        episode_reward = 0
        observation, _ = self.reset()
        action = self.agent_resemble_decide(observation)

        done = False
        while True:
            if self.render:
                self.env.render()

            next_observation, reward, terminated, truncated, _ = self.step(action)
            episode_reward += reward
            # logger.info(f"å½“å‰çŠ¶æ€:{next_observation}")
            next_action = self.agent_resemble_decide(next_observation)
            # logger.info(f"é€‰æ‹©åŠ¨ä½œ:{next_action}")

            if terminated or truncated:
                done = True

            if train:
                self.sarsa_resemble_learn(observation, action, reward, next_observation, next_action, done)
            else:
                time.sleep(2)

            if done:
                logger.info(f"ç»“æŸä¸€è½®æ¸¸æˆ")
                break
            observation, action = next_observation, next_action
        return episode_reward


class SARSALamdaAgent(EnvInit):
    """
    å‡½æ•°è¿‘ä¼¼SARSA(ğœ†)ç®—æ³•
    """
    def __init__(self, lamda=0.9, layers=8, features=1893):
        super().__init__(render = True)
        self.lamda = lamda
        self.layers = layers
        self.features = features
        self.obs_low = self.env.observation_space.low
        self.obs_scale = self.observation_space.high - self.env.observation_space.low
        self.z = np.zeros(features)
        # åŠ è½½æ¨¡å‹
        self.train = True
        self.load_model = True
        if not self.load_model:
            self.encoder = TileCoder(self.layers, self.features)
            self.w = np.zeros(features)
        else:
            self.w, codebook = Policy_loader.load_w_para(class_name=self.__class__.__name__,
                                                         method_name="play_game_by_sarsa_lamda.pkl")
            self.encoder = TileCoder(self.layers, self.features, codebook)

    def encode(self, observation, action):
        """
        ç¼–ç 
        """
        states = tuple((observation - self.obs_low) / self.obs_scale)
        actions = (action,)
        return self.encoder(states, actions)

    def get_q(self, observation, action):
        """
        è·å–åŠ¨ä½œä»·å€¼
        :param observation:
        :param action:
        :return:
        """
        features = self.encode(observation, action)
        # logger.info(f"features:{features}")
        return self.w[features].sum()

    def agent_resemble_decide(self, observation):
        """
        å†³ç­–
        :param observation:
        :return:
        """
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.Action_Num)
        else:
            qs = [self.get_q(observation, action) for action in range(self.Action_Num)]
            return np.argmax(qs)

    def SARSA_Lamda_learn(self, observation, action, reward, next_observation, next_action, done):
        u = reward
        if not done:
            u += (self.gamma * self.get_q(next_observation, next_action))
            self.z *= (self.gamma * self.lamda)
            features = self.encode(observation, action)
            self.z[features] = 1.
        td_error = u - self.get_q(observation, action)
        self.w += (self.learning_rate * td_error * self.z)
        if done:
            self.z = np.zeros_like(self.z)

    def play_game_by_sarsa_lamda(self, train=False):
        """
        ä½¿ç”¨SARSAç®—æ³•è®­ç»ƒ
        :param train:
        :return:
        """
        episode_reward = 0
        observation, _ = self.reset()
        action = self.agent_resemble_decide(observation)
        done = False
        while True:
            if self.render:
                self.env.render()

            next_observation, reward, terminated, truncated, _ = self.step(action)
            # taxi_row, taxi_col, pass_loc, dest_idx = self.envs.decode(next_observation)

            # if not train:
            #     logger.info(f"ä¸‹ä¸€ä¸ªçŠ¶æ€ï¼š{(taxi_row, taxi_col)}")
            episode_reward += reward

            next_action = self.agent_resemble_decide(next_observation)

            # if not train:
            #     logger.info(f"ä¸‹ä¸€ä¸ªåŠ¨ä½œï¼š{self.translate(action)}")

            if terminated or truncated:
                done = True

            if train:
                self.SARSA_Lamda_learn(observation, action, reward, next_observation, next_action, done)
            else:
                time.sleep(2)

            if done:
                logger.info(f"ç»“æŸä¸€è½®æ¸¸æˆ")
                break
            observation, action = next_observation, next_action
        return episode_reward


class MountainCar(SARSAAgent, SARSALamdaAgent):
    def __init__(self):
        SARSAAgent.__init__(self)
        SARSALamdaAgent.__init__(self)
        self.class_name = self.__class__.__name__

    def play_game(self):
        """
        æ™ºèƒ½ä½“æ¨æ¼”
        :return:
        """
        self.print_env_info()
        observation, _ = self.reset()
        while True:
            self.positions.append(observation[0])
            self.velocities.append(observation[1])
            next_observation, reward, terminated, truncated, _ = self.step(2)
            done = terminated or truncated
            if done:
                break
            observation = next_observation

        if next_observation[0] > 0.5:
            logger.info("æˆåŠŸ")
        else:
            logger.info("å¤±è´¥")

        Visualizer.plot_maintain_curve(self.positions, self.velocities)

    def game_iteration(self, show_policy):
        """
        è¿­ä»£
        :param show_policy: ä½¿ç”¨çš„æ›´æ–°ç­–ç•¥æ–¹å¼
        """
        episode_reward = 0.
        episode_rewards = []  # æ€»è½®æ•°çš„å¥–åŠ±(æŸè½®æ€»å¥–åŠ±)åˆ—è¡¨

        method_name = "default"

        for game_round in range(1, self.game_rounds):
            logger.info(f"---ç¬¬{game_round}è½®è®­ç»ƒ---")
            if show_policy == "å‡½æ•°è¿‘ä¼¼SARSAç®—æ³•":
                # logger.info(f"å‡½æ•°è¿‘ä¼¼SARSAç®—æ³•")
                episode_reward = self.play_game_by_sarsa_resemble(train=True)  # ç¬¬roundè½®æ¬¡çš„ç´¯ç§¯reward
                method_name = self.play_game_by_sarsa_resemble.__name__
            if show_policy == "å‡½æ•°è¿‘ä¼¼SARSA(ğœ†)ç®—æ³•":
                # logger.info(f"å‡½æ•°è¿‘ä¼¼SARSA(ğœ†)ç®—æ³•")
                episode_reward = self.play_game_by_sarsa_lamda(train=True)  # ç¬¬roundè½®æ¬¡çš„ç´¯ç§¯reward
                method_name = self.play_game_by_sarsa_lamda.__name__

            if self.save_policy:
                save_data = {
                    "weights": self.w,
                    "encoder": self.encoder.codebook if self.encoder else None
                }
                Policy_loader.save_policy(method_name, self.class_name, save_data)

            if episode_reward is not None:
                episode_rewards.append(episode_reward)
                logger.info(f"ç¬¬{game_round}è½®å¥–åŠ±: {episode_reward}")
            else:
                logger.warning(f"ç¬¬{game_round}è½®å¥–åŠ±ä¸º Noneï¼Œå·²è·³è¿‡ã€‚")

            Visualizer.plot_cumulative_avg_rewards(episode_rewards, game_round, self.game_rounds, self.class_name,
                                                   method_name)

        print(
            f"å¹³å‡å¥–åŠ±ï¼š{(np.round(np.mean(episode_rewards), 2))} = {np.sum(episode_rewards)} / {len(episode_rewards)}")
        print(
            f"æœ€å100è½®å¥–åŠ±ï¼š{(np.round(np.mean(episode_rewards[-500:]), 2))} = {np.sum(episode_rewards[-500:])} / {len(episode_rewards[-500:])}")
