# -*- coding: utf-8 -*-
"""
@File    : cartpole.py
@Time    : 2025/2/1 14:00
@Author  : zhangjian
@Email   : your_email@example.com
@Desc    : 
"""
import time
from collections import deque

import torch
import gym
import numpy as np
import logging
import torch.nn as nn
import pandas as pd
from torch.utils.tensorboard import SummaryWriter
from envs.env_template import Env
from tools.visualizer import Visualizer
from tools.save_policy import Policy_loader
import torch.optim as optim

logger = logging.getLogger(__name__)  # ä½¿ç”¨å½“å‰æ¨¡å—å
from envs.global_set import *


class EnvInit(Env):
    """
    ç®—æ³•å‚æ•°åˆå§‹åŒ–
    """

    def __init__(self, name='CartPole-v0', render_mode=render_model[0], render=True):
        super().__init__(name, render_mode, render)
        # æ˜¯å¦å¼€å¯åŠ¨ç”»
        if render:
            self.env = gym.make(name, render_mode=render_mode)
        else:
            self.env = gym.make(name)

        self.render = render
        # æ¸¸æˆè½®æ•°
        self.game_rounds = 20000
        # è·å–åŠ¨ä½œç©ºé—´çš„å¤§å°ï¼Œå³å¯é€‰æ‹©çš„åŠ¨ä½œæ•°é‡
        self.Action_Num = self.env.action_space.n
        # ä½ç½®
        self.positions = []
        # ç”¨äºè·Ÿè¸ªæœ€è¿‘æ¸¸æˆçš„å®Œæˆç‡
        self.done_rate = deque(maxlen=100)
        self.done_rate.clear()
        # é€Ÿåº¦
        self.velocities = []
        # ä¿å­˜æ¨¡å‹
        self.save_policy = False
        # åŠ è½½æ¨¡å‹
        self.load_model = True
        # æ˜¯å¦å¼€å¯tensorboardè®°å½•logs
        self.is_open_writer = True
        # æ˜¯å¦å…¨å±€è®­ç»ƒï¼Œç”¨äºè®¾ç½®æŸäº›è®°å½•
        self.global_is_train = False
        # æŠ˜æ‰£å› å­ï¼Œå†³å®šäº†æœªæ¥å¥–åŠ±çš„å½±å“
        self.gamma = 1.
        # å­¦ä¹ ç‡
        self.learning_rate = 0.01
        # æŸ¯è¥¿æ”¶æ•›èŒƒå›´
        self.tolerant = 1e-6
        # Îµ-æŸ”æ€§ç­–ç•¥å› å­
        self.epsilon = 0.001
        self.translate_action = {
            0: "å·¦",
            1: "æ— ",
            2: "å³"
        }


class BuildNetwork(nn.Module):
    def __init__(self, hidden_sizes, output_size, activation=nn.ReLU, output_activation=None):
        super(BuildNetwork, self).__init__()
        in_features = 4  # è¾“å…¥çŠ¶æ€ç©ºé—´ç»´åº¦
        layers = []

        # æ„å»ºéšè—å±‚
        for i, hidden_size in enumerate(hidden_sizes):
            # å¯¹äºç¬¬ä¸€å±‚ï¼Œæˆ‘ä»¬éœ€è¦æ ¹æ®è¾“å…¥ç»´åº¦æ¥è®¾ç½® in_features
            layers.append(nn.Linear(in_features=in_features, out_features=hidden_size))
            layers.append(activation())  # ä½¿ç”¨æŒ‡å®šçš„æ¿€æ´»å‡½æ•°
            in_features = hidden_size

        # æ„å»ºè¾“å‡ºå±‚
        layers.append(nn.Linear(in_features=hidden_sizes[-1], out_features=output_size))
        if output_activation:
            layers.append(output_activation())  # å¦‚æœæœ‰è¾“å‡ºæ¿€æ´»å‡½æ•°

        # å°†å±‚ç»„åˆæˆä¸€ä¸ªç½‘ç»œ
        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

    def get_optimizer(self, learning_rate):
        return optim.Adam(self.parameters(), lr=learning_rate)


class VPGAgent(EnvInit):
    def __init__(self, gamma=0.99, learning_rate=0.001):
        """
        åŒç­–ç­–ç•¥æ¢¯åº¦
        :param gamma:
        :param learning_rate:
        """
        super().__init__()
        self.gamma = gamma
        # å…¶ä»–è¶…å‚æ•°
        self.learn_step_counter = int(0)  # å­¦ä¹ æ­¥è®¡æ•°å™¨
        self.learning_rate = 0.0001  # å­¦ä¹ ç‡
        self.goal_position = 0.5
        self.replay_start_size = 1000  # ç»éªŒæ± å¼€å§‹è®­ç»ƒæ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°é‡
        self.update_lr_steps = 5000  # å­¦ä¹ ç‡åˆ·æ–°é—´éš”
        current_time = time.localtime()
        log_dir = time.strftime("runs/vpg_agent/%Y_%m_%d_%H_%M", current_time)
        if self.is_open_writer:
            self.writer = SummaryWriter(log_dir=log_dir)
        self.trajectory = []
        policy_kwargs = {
            'hidden_sizes': [10, ],  # éšè—å±‚å¤§å°
            'output_size': 10,  # è¾“å‡ºç±»åˆ«æ•°é‡
        }
        baseline_kwargs = {
            'hidden_sizes': [10, ],  # åŸºçº¿ç½‘ç»œçš„éšè—å±‚
        }
        # æ„å»ºç­–ç•¥ç½‘ç»œ
        self.policy_net = BuildNetwork(
            hidden_sizes=policy_kwargs['hidden_sizes'],
            output_size=self.Action_Num,
            activation=policy_kwargs.get('activation', nn.ReLU),
            output_activation=policy_kwargs.get('output_activation', nn.Softmax)
        )
        self.policy_optimizer = self.policy_net.get_optimizer(learning_rate)

        if baseline_kwargs:
            # æ„å»ºåŸºçº¿ç½‘ç»œ
            self.baseline_net = BuildNetwork(
                hidden_sizes=baseline_kwargs['hidden_sizes'],
                output_size=1,  # åŸºçº¿ç½‘ç»œè¾“å‡ºä¸€ä¸ªå€¼
                activation=baseline_kwargs.get('activation', nn.ReLU)
            )
            self.baseline_optimizer = self.baseline_net.get_optimizer(learning_rate)

        # å¦‚æœåŠ è½½æ¨¡å‹
        if self.load_model:
            checkpoint = torch.load("tools/policy_dir/CartPole/policy_net.pth", weights_only=True)
            self.policy_net.load_state_dict(checkpoint["model_state_dict"])
            self.policy_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            logger.info(f"æˆåŠŸåŠ è½½--->policy_net")
            checkpoint = torch.load("tools/policy_dir/CartPole/baseline_net.pth", weights_only=True)
            self.baseline_net.load_state_dict(checkpoint["model_state_dict"])
            self.baseline_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            logger.info(f"æˆåŠŸåŠ è½½--->baseline_net")

    def vpg_decide(self, observation):
        """
        å†³ç­–
        :param observation:
        :return:
        """
        # ç¡®ä¿ observation æ˜¯ PyTorch tensorï¼Œå¹¶ä¸”æ·»åŠ  batch ç»´åº¦
        # unsqueeze(0) å°†ä¼šåœ¨ç¬¬ä¸€ç»´æ·»åŠ ä¸€ä¸ªæ–°çš„ç»´åº¦ï¼Œå½¢çŠ¶å˜ä¸º (1, 4)
        observation = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)  # å½¢çŠ¶å˜ä¸º (1, observation_size)
        # è·å–ç­–ç•¥ç½‘ç»œçš„è¾“å‡ºï¼ˆlogitsï¼‰
        probs = self.policy_net(observation)  # è¾“å‡ºçš„å¤§å°ä¸º (1, Action_Num)
        # å°†æ¦‚ç‡å€¼è½¬æ¢ä¸º NumPy æ•°ç»„ï¼Œå¹¶è¿›è¡Œéšæœºé€‰æ‹©åŠ¨ä½œ
        """
        1. detach() çš„ä½œç”¨æ˜¯ä»è®¡ç®—å›¾ä¸­åˆ†ç¦»å‡º probsï¼Œå³å®ƒä¸å†å‚ä¸åç»­çš„æ¢¯åº¦è®¡ç®—ã€‚
        2. cpu() å°† probs å¼ é‡ä»å½“å‰è®¾å¤‡ï¼ˆæ¯”å¦‚ GPUï¼‰ç§»åŠ¨åˆ° CPU ä¸Šï¼Œ
        è¿™å¯¹äºåç»­çš„ numpy() è½¬æ¢æ˜¯å¿…éœ€çš„ï¼Œå› ä¸º numpy() ä¸æ”¯æŒç›´æ¥æ“ä½œ GPU ä¸Šçš„å¼ é‡
        3. æœ€åï¼Œnumpy() å°† PyTorch çš„å¼ é‡è½¬æ¢ä¸º NumPy æ•°ç»„ï¼ŒNumPy æ˜¯ Python ä¸­å¸¸ç”¨çš„æ•°ç»„åº“ï¼Œ
        ä¸æ”¯æŒç›´æ¥ä¸ PyTorch å¼ é‡è¿›è¡Œè®¡ç®—ï¼Œæ‰€ä»¥éœ€è¦è½¬æ¢ä¸º NumPy æ•°ç»„ã€‚
        """
        probs = probs.squeeze(0).detach().cpu().numpy()  # .squeeze(0) å»æ‰ batch ç»´åº¦ï¼Œè½¬æ¢ä¸º (Action_Num,)
        # æ ¹æ®æ¦‚ç‡åˆ†å¸ƒé€‰æ‹©åŠ¨ä½œ
        action = np.random.choice(self.Action_Num, p=probs)

        return action

    def vpg_learn(self, observation, action, reward, done):
        self.trajectory.append((observation, action, reward))
        # å› ä¸ºæ˜¯å¹³è¡¡æ¸¸æˆï¼Œæ‰€ä»¥åªæœ‰å½“æ¸¸æˆå¼ºè¡Œåœæ­¢æ—¶(ä¹Ÿå°±æ˜¯è¾¾åˆ°200æ­¥éª¤)ï¼Œæ‰ä¼šç»“æŸ
        if done:
            # å°†è½¨è¿¹è½¬æ¢ä¸º Pandas DataFrame
            df = pd.DataFrame(self.trajectory, columns=['observation', 'action', 'reward'])
            # df.index.to_series()ä¼šå°†ç´¢å¼•è½¬æ¢ä¸ºä¸€ä¸ª Series å¯¹è±¡ï¼Œ=0,1,2,...,200ï¼Œå°†ç´¢å¼•ä½œä¸ºå¹‚æ¬¡æ–¹å¯¹è±¡
            df['discount'] = self.gamma ** df.index.to_series()
            # df['discounted_reward']ç›¸å½“äºæ¯ä¸ªå¥–åŠ±éƒ½ä¼šå¸¦ä¸€ä¸ªæŠ˜æ‰£å› å­
            df['discounted_reward'] = df['discount'] * df['reward']
            # å°†æŠ˜æ‰£å¥–åŠ±åºåˆ—åè½¬ï¼Œè¡¨ç¤ºä»ç»ˆæ­¢çŠ¶æ€åˆ°å¼€å§‹çŠ¶æ€çš„é¡ºåºã€‚å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œé€šå¸¸ä»ç»ˆæ­¢çŠ¶æ€åå‘è®¡ç®—å›æŠ¥ã€‚
            # .cumsum() æ˜¯ Pandas ä¸­è®¡ç®—ç´¯ç§¯å’Œçš„å‡½æ•°ã€‚åœ¨è¿™é‡Œï¼Œå®ƒç”¨äºè®¡ç®—ä»åå‘é¡ºåºçš„æŠ˜æ‰£å¥–åŠ±åºåˆ—çš„ç´¯ç§¯å’Œã€‚
            # ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯ä¸€æ­¥çš„æŠ˜æ‰£ç´¯ç§¯å›æŠ¥ï¼ˆdiscounted_returnï¼‰æ˜¯ä»åé¢çš„å¥–åŠ±å¼€å§‹åŠ æƒç´¯åŠ çš„ã€‚
            df['discounted_return'] = df['discounted_reward'][::-1].cumsum() # ç¬¬ä¸€é¡¹å°±å­˜åœ¨æŠ˜æ‰£å› å­
            df['psi'] = df['discounted_return']

            # å°†è¾“å…¥è½¬æ¢ä¸º Tensorï¼ˆ200ï¼Œ4ï¼‰
            state = torch.tensor(np.stack(df['observation']), dtype=torch.float32)

            # å¦‚æœæœ‰åŸºçº¿ç½‘ç»œ
            # æ£€æŸ¥å½“å‰å¯¹è±¡ï¼ˆselfï¼‰æ˜¯å¦åŒ…å« baseline_net å±æ€§
            if hasattr(self, 'baseline_net'):
                baseline_output = self.baseline_net(state)  # è¾“å‡ºä¸€ä¸ªåŸºçº¿å€¼ï¼ˆ200ï¼Œ1ï¼‰
                # æ¯ä¸ªçŠ¶æ€çš„å€¼å‡½æ•°ä¼°è®¡
                df['baseline'] = baseline_output.detach().numpy()  # detach() ä¸è¿›è¡Œæ¢¯åº¦è®¡ç®—
                # ä¼˜åŠ¿å‡½æ•°ï¼Œå› ä¸ºåŸºçº¿ç½‘ç»œçš„è¾“å‡ºæ˜¯çŠ¶æ€ä»·å€¼ï¼Œç¬¬ä¸€é¡¹æ²¡æœ‰æŠ˜æ‰£å› å­ï¼Œä¸ºäº†ä¸df['psi']åŒ¹é…éœ€è¦ä¹˜ä¸Š
                df['psi'] -= (df['baseline'].squeeze() * df['discount'])
                # è¿™é‡Œè®¡ç®— df['return'] åˆ—ï¼Œå®ƒé€šå¸¸è¡¨ç¤º æ ‡å‡†åŒ–çš„å›æŠ¥ï¼Œä½¿ç”¨ é™¤ä»¥æŠ˜æ‰£å› å­ï¼Œè¿™æ˜¯ä¸ºäº†æ¶ˆé™¤æŠ˜æ‰£å› å­çš„å½±å“å¹¶ä½¿å›æŠ¥æ¢å¤åˆ°æ¥è¿‘äºâ€œæœªç»æŠ˜æ‰£çš„å›æŠ¥â€
                df['return'] = df['discounted_return'] / df['discount']
                # df['return'].values è·å– return åˆ—çš„æ•°æ®ã€‚G
                G = torch.tensor(df['return'].values, dtype=torch.float32).unsqueeze(1)

                # åŸºçº¿ç½‘ç»œçš„è®­ç»ƒ
                self.baseline_optimizer.zero_grad()
                V_s = self.baseline_net(state)  # çŠ¶æ€ä»·å€¼ä¼°è®¡:v(S;w)
                baseline_loss = nn.MSELoss()(V_s, G)  # V_s:é¢„æµ‹(çŠ¶æ€ä¼°è®¡)ï¼Œ G:å®é™…æ ‡å‡†å›æŠ¥
                baseline_loss.backward()
                self.baseline_optimizer.step()

            # ç­–ç•¥ç½‘ç»œè®­ç»ƒï¼Œdf['psi'].valuesï¼špsi åˆ—çš„numpyæ•°æ®ï¼Œæ— è®ºæ˜¯å¦å½’ä¸€åŒ–ï¼Œä»£è¡¨çš„æ˜¯ä¸€ç§ç›¸å¯¹å½±å“ï¼Œåªè¦ä½“ç°æ­£è´Ÿå…³ç³»å³å¯ã€‚
            advantage = torch.tensor(df['psi'].values, dtype=torch.float32)

            self.policy_optimizer.zero_grad()

            # è®¡ç®—ç­–ç•¥ç½‘ç»œçš„è¾“å‡ºï¼Œğ›‘(a|s)
            policy_output = self.policy_net(state)

            # ä½¿ç”¨è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±,ç”±äºç­–ç•¥æ¢¯åº¦æ–¹æ³•é€šå¸¸ä¼šä½¿ç”¨ å¯¹æ•°æ¦‚ç‡ æ¥é¿å…æ¦‚ç‡å€¼éå¸¸å°æ—¶çš„æ•°å€¼ç¨³å®šæ€§é—®é¢˜ï¼Œ
            # å› æ­¤è¿™é‡Œå¯¹ policy_outputï¼ˆåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒï¼‰å–å¯¹æ•°
            log_probs = torch.log(policy_output)
            # gather: å®ƒä»ç»™å®šç»´åº¦ dim ä¸Šæ ¹æ®æŒ‡å®šçš„ index é€‰æ‹©å¯¹åº”çš„å€¼
            # gather(1, ...) è¡¨ç¤ºæˆ‘ä»¬ä» log_probs ä¸­æŒ‰åˆ—é€‰æ‹©ç‰¹å®šçš„åŠ¨ä½œæ¦‚ç‡,df['action'].values è¡¨ç¤ºactionä¸­çš„å€¼-åŠ¨ä½œçš„ç´¢å¼•
            # view(-1, 1)ï¼šï¼ˆ-1ï¼‰è¡Œè‡ªåŠ¨æ¨æ–­ï¼Œåˆ—ä¸º1åˆ—
            selected_log_probs = log_probs.gather(1, torch.tensor(df['action'].values, dtype=torch.long).view(-1, 1))
            # æœ€å°åŒ–è´ŸæœŸæœ›å›æŠ¥
            policy_loss = -(selected_log_probs * advantage.view(-1, 1)).mean()

            policy_loss.backward()
            self.policy_optimizer.step()

            # æ¸…ç©ºè½¨è¿¹
            self.trajectory = []

    def play_montecarlo(self, train=False):
        """
        ä½¿ç”¨ DQN ç®—æ³•è®­ç»ƒå’Œè¯„ä¼°
        :param train: æ˜¯å¦è®­ç»ƒæ¨¡å¼
        :return: ç´¯ç§¯å¥–åŠ±
        """
        episode_reward = 0
        observation, _ = self.reset()
        done = False

        if not train:
            logger.info(f"****å¯åŠ¨è¯„ä¼°é˜¶æ®µ****")
            self.policy_net.eval()
            self.baseline_net.eval()

        while True:
            if self.render:
                self.env.render()

            if not train:
                with torch.no_grad():
                    action = self.vpg_decide(observation)
            else:
                action = self.vpg_decide(observation)

            next_observation, reward, terminated, truncated, _ = self.step(action)
            episode_reward += reward

            if terminated or truncated:
                done = True
                # self.learn_step_counter += 1

            if train:
                self.vpg_learn(observation, action, reward, done)
            if done:
                logger.info(f"ç»“æŸä¸€è½®æ¸¸æˆ, å¥–åŠ±ä¸º${episode_reward}")
                flag = True if episode_reward >= 195 else False
                self.done_rate.append(flag)
                break

            observation = next_observation
        return episode_reward


class OffPolicyVPGAgent(EnvInit):
    def __init__(self, gamma=0.99, learning_rate=0.001):
        """
        å¼‚ç­–ç­–ç•¥æ¢¯åº¦
        :param gamma:
        :param learning_rate:
        """
        super().__init__()
        self.gamma = gamma
        # å…¶ä»–è¶…å‚æ•°
        self.learn_step_counter = int(0)  # å­¦ä¹ æ­¥è®¡æ•°å™¨
        self.learning_rate = 0.00001  # å­¦ä¹ ç‡
        self.goal_position = 0.5
        self.replay_start_size = 1000  # ç»éªŒæ± å¼€å§‹è®­ç»ƒæ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°é‡
        self.update_lr_steps = 5000  # å­¦ä¹ ç‡åˆ·æ–°é—´éš”

        current_time = time.localtime()
        log_dir = time.strftime("runs/off_vpg_agent/%Y_%m_%d_%H_%M", current_time)
        if self.is_open_writer:
            self.writer = SummaryWriter(log_dir=log_dir)

        self.trajectory = []

        def dot(y_true, y_pred):
            return -torch.sum(y_true * y_pred, dim=-1)

        policy_kwargs = {
            'hidden_sizes': [10, ],  # éšè—å±‚å¤§å°
            'output_size': 10,  # è¾“å‡ºç±»åˆ«æ•°é‡
        }
        baseline_kwargs = {
            'hidden_sizes': [10, ],  # åŸºçº¿ç½‘ç»œçš„éšè—å±‚
        }
        # æ„å»ºç­–ç•¥ç½‘ç»œ
        self.off_policy_net = BuildNetwork(
            hidden_sizes=policy_kwargs['hidden_sizes'],
            output_size=self.Action_Num,
            activation=policy_kwargs.get('activation', nn.ReLU),
            output_activation=policy_kwargs.get('output_activation', nn.Softmax)
        )
        self.off_policy_optimizer = self.off_policy_net.get_optimizer(learning_rate)

        if baseline_kwargs:
            # æ„å»ºåŸºçº¿ç½‘ç»œ
            self.off_baseline_net = BuildNetwork(
                hidden_sizes=baseline_kwargs['hidden_sizes'],
                output_size=1,  # åŸºçº¿ç½‘ç»œè¾“å‡ºä¸€ä¸ªå€¼
                activation=baseline_kwargs.get('activation', nn.ReLU)
            )
            self.off_baseline_optimizer = self.off_baseline_net.get_optimizer(learning_rate)

        # å¦‚æœåŠ è½½æ¨¡å‹
        if self.load_model:
            checkpoint = torch.load("tools/policy_dir/CartPole/off_policy_net.pth", weights_only=True)
            self.off_policy_net.load_state_dict(checkpoint["model_state_dict"])
            self.off_policy_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            logger.info(f"æˆåŠŸåŠ è½½--->off_policy_net")
            checkpoint = torch.load("tools/policy_dir/CartPole/off_baseline_net.pth", weights_only=True)
            self.off_baseline_net.load_state_dict(checkpoint["model_state_dict"])
            self.off_baseline_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            logger.info(f"æˆåŠŸåŠ è½½--->off_baseline_net")

    def off_vpg_learn(self, observation, action, behavior, reward, done):
        self.trajectory.append((observation, action, behavior, reward))

        if done:
            # å°†è½¨è¿¹è½¬æ¢ä¸º Pandas DataFrame
            df = pd.DataFrame(self.trajectory, columns=['observation', 'action', 'behavior', 'reward'])
            # ä¼šå°†ç´¢å¼•è½¬æ¢ä¸ºä¸€ä¸ª Series å¯¹è±¡ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ è¡¨ç¤ºè½¨è¿¹çš„ä¸€ä¸ªæ—¶é—´æ­¥
            df['discount'] = self.gamma ** df.index.to_series()
            df['discounted_reward'] = df['discount'] * df['reward']
            # å°†æŠ˜æ‰£å¥–åŠ±åºåˆ—åè½¬ï¼Œè¡¨ç¤ºä»ç»ˆæ­¢çŠ¶æ€åˆ°å¼€å§‹çŠ¶æ€çš„é¡ºåºã€‚å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œé€šå¸¸ä»ç»ˆæ­¢çŠ¶æ€åå‘è®¡ç®—å›æŠ¥ã€‚
            # .cumsum() æ˜¯ Pandas ä¸­è®¡ç®—ç´¯ç§¯å’Œçš„å‡½æ•°ã€‚åœ¨è¿™é‡Œï¼Œå®ƒç”¨äºè®¡ç®—ä»åå‘é¡ºåºçš„æŠ˜æ‰£å¥–åŠ±åºåˆ—çš„ç´¯ç§¯å’Œã€‚
            # ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯ä¸€æ­¥çš„æŠ˜æ‰£ç´¯ç§¯å›æŠ¥ï¼ˆdiscounted_returnï¼‰æ˜¯ä»åé¢çš„å¥–åŠ±å¼€å§‹åŠ æƒç´¯åŠ çš„ã€‚
            df['discounted_return'] = df['discounted_reward'][::-1].cumsum()
            df['psi'] = df['discounted_return']

            # å°†è¾“å…¥è½¬æ¢ä¸º Tensorï¼ˆ200ï¼Œ4ï¼‰
            state = torch.tensor(np.stack(df['observation']), dtype=torch.float32)

            # å¦‚æœæœ‰åŸºçº¿ç½‘ç»œ
            # æ£€æŸ¥å½“å‰å¯¹è±¡ï¼ˆselfï¼‰æ˜¯å¦åŒ…å« baseline_net å±æ€§
            if hasattr(self, 'baseline_net'):
                baseline_output = self.baseline_net(state)  # è¾“å‡ºä¸€ä¸ªåŸºçº¿å€¼ï¼ˆ200ï¼Œ1ï¼‰
                # æ¯ä¸ªçŠ¶æ€çš„å€¼å‡½æ•°ä¼°è®¡
                df['baseline'] = baseline_output.detach().numpy()  # detach() ä¸è¿›è¡Œæ¢¯åº¦è®¡ç®—
                # ä¼˜åŠ¿å‡½æ•°
                df['psi'] -= (df['baseline'].squeeze() * df['discount'])
                # è¿™é‡Œè®¡ç®— df['return'] åˆ—ï¼Œå®ƒé€šå¸¸è¡¨ç¤º æ ‡å‡†åŒ–çš„å›æŠ¥ï¼Œä½¿ç”¨ é™¤ä»¥æŠ˜æ‰£å› å­ï¼Œè¿™æ˜¯ä¸ºäº†æ¶ˆé™¤æŠ˜æ‰£å› å­çš„å½±å“å¹¶ä½¿å›æŠ¥æ¢å¤åˆ°æ¥è¿‘äºâ€œæœªç»æŠ˜æ‰£çš„å›æŠ¥â€
                df['return'] = df['discounted_return'] / df['discount']
                # df['return'].values è·å– return åˆ—çš„æ•°æ®ã€‚G
                G = torch.tensor(df['return'].values, dtype=torch.float32).unsqueeze(1)

                # åŸºçº¿ç½‘ç»œçš„è®­ç»ƒ
                self.off_baseline_optimizer.zero_grad()
                V_s = self.baseline_net(state)  # çŠ¶æ€ä»·å€¼ä¼°è®¡:v(S;w)
                baseline_loss = nn.MSELoss()(V_s, G)  # V_s:é¢„æµ‹(çŠ¶æ€ä¼°è®¡)ï¼Œ G:å®é™…æ ‡å‡†å›æŠ¥
                baseline_loss.backward()
                self.off_baseline_optimizer.step()

            # ç­–ç•¥ç½‘ç»œè®­ç»ƒï¼Œdf['psi'].valuesï¼špsi åˆ—çš„numpyæ•°æ®
            y = torch.tensor((df['psi'] / df['behavior']).values, dtype=torch.float32)

            self.off_policy_optimizer.zero_grad()

            # è®¡ç®—ç­–ç•¥ç½‘ç»œçš„è¾“å‡º
            policy_output = self.off_policy_net(state)

            # ä½¿ç”¨è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±,ç”±äºç­–ç•¥æ¢¯åº¦æ–¹æ³•é€šå¸¸ä¼šä½¿ç”¨ å¯¹æ•°æ¦‚ç‡ æ¥é¿å…æ¦‚ç‡å€¼éå¸¸å°æ—¶çš„æ•°å€¼ç¨³å®šæ€§é—®é¢˜ï¼Œ
            # å› æ­¤è¿™é‡Œå¯¹ policy_outputï¼ˆåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒï¼‰å–å¯¹æ•°
            log_probs = torch.log(policy_output)
            # gather: å®ƒä»ç»™å®šç»´åº¦ dim ä¸Šæ ¹æ®æŒ‡å®šçš„ index é€‰æ‹©å¯¹åº”çš„å€¼
            # gather(1, ...) è¡¨ç¤ºæˆ‘ä»¬ä» log_probs ä¸­æŒ‰åˆ—é€‰æ‹©ç‰¹å®šçš„åŠ¨ä½œæ¦‚ç‡,df['action'].values è¡¨ç¤ºåŠ¨ä½œçš„ç´¢å¼•
            # view(-1, 1)ï¼šè¡Œè‡ªåŠ¨æ¨æ–­ï¼Œåˆ—ä¸ºä¸€åˆ—

            selected_log_probs = log_probs.gather(1, torch.tensor(df['action'].values, dtype=torch.long).view(-1, 1))

            # æœ€å°åŒ–è´ŸæœŸæœ›å›æŠ¥
            policy_loss = -(selected_log_probs * y.view(-1, 1)).mean()

            policy_loss.backward()
            self.off_policy_optimizer.step()

            # æ¸…ç©ºè½¨è¿¹
            self.trajectory = []

    def off_decide(self, observation):
        # ç¡®ä¿ observation æ˜¯ PyTorch tensorï¼Œå¹¶ä¸”æ·»åŠ  batch ç»´åº¦
        # unsqueeze(0) å°†ä¼šåœ¨ç¬¬ä¸€ç»´æ·»åŠ ä¸€ä¸ªæ–°çš„ç»´åº¦ï¼Œå½¢çŠ¶å˜ä¸º (1, 4)
        observation = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)  # å½¢çŠ¶å˜ä¸º (1, observation_size)
        # è·å–ç­–ç•¥ç½‘ç»œçš„è¾“å‡ºï¼ˆlogitsï¼‰
        probs = self.off_policy_net(observation)  # è¾“å‡ºçš„å¤§å°ä¸º (1, Action_Num)
        # å°†æ¦‚ç‡å€¼è½¬æ¢ä¸º NumPy æ•°ç»„ï¼Œå¹¶è¿›è¡Œéšæœºé€‰æ‹©åŠ¨ä½œ
        """
        1. detach() çš„ä½œç”¨æ˜¯ä»è®¡ç®—å›¾ä¸­åˆ†ç¦»å‡º probsï¼Œå³å®ƒä¸å†å‚ä¸åç»­çš„æ¢¯åº¦è®¡ç®—ã€‚
        2. cpu() å°† probs å¼ é‡ä»å½“å‰è®¾å¤‡ï¼ˆæ¯”å¦‚ GPUï¼‰ç§»åŠ¨åˆ° CPU ä¸Šï¼Œ
        è¿™å¯¹äºåç»­çš„ numpy() è½¬æ¢æ˜¯å¿…éœ€çš„ï¼Œå› ä¸º numpy() ä¸æ”¯æŒç›´æ¥æ“ä½œ GPU ä¸Šçš„å¼ é‡
        3. æœ€åï¼Œnumpy() å°† PyTorch çš„å¼ é‡è½¬æ¢ä¸º NumPy æ•°ç»„ï¼ŒNumPy æ˜¯ Python ä¸­å¸¸ç”¨çš„æ•°ç»„åº“ï¼Œ
        ä¸æ”¯æŒç›´æ¥ä¸ PyTorch å¼ é‡è¿›è¡Œè®¡ç®—ï¼Œæ‰€ä»¥éœ€è¦è½¬æ¢ä¸º NumPy æ•°ç»„ã€‚
        """
        probs = probs.squeeze(0).detach().cpu().numpy()  # .squeeze(0) å»æ‰ batch ç»´åº¦ï¼Œè½¬æ¢ä¸º (Action_Num,)
        # æ ¹æ®æ¦‚ç‡åˆ†å¸ƒé€‰æ‹©åŠ¨ä½œ
        action = np.random.choice(self.Action_Num, p=probs)
        behavior = 1. / self.Action_Num
        return action, behavior

    def off_play_montecarlo(self, train=False):
        """
        ä½¿ç”¨ DQN ç®—æ³•è®­ç»ƒå’Œè¯„ä¼°
        :param train: æ˜¯å¦è®­ç»ƒæ¨¡å¼
        :return: ç´¯ç§¯å¥–åŠ±
        """
        episode_reward = 0
        observation, _ = self.reset()
        done = False

        if not train:
            logger.info(f"****å¯åŠ¨è¯„ä¼°é˜¶æ®µ****")
            self.off_policy_net.eval()
            self.off_baseline_net.eval()

        while True:
            if self.render:
                self.env.render()

            if not train:
                with torch.no_grad():
                    action, behavior = self.off_decide(observation)
            else:
                action, behavior = self.off_decide(observation)

            next_observation, reward, terminated, truncated, _ = self.step(action)
            episode_reward += reward

            if terminated or truncated:
                done = True
                # self.learn_step_counter += 1

            if train:
                self.off_vpg_learn(observation, action, behavior, reward, done)
            if done:
                logger.info(f"ç»“æŸä¸€è½®æ¸¸æˆ, å¥–åŠ±ä¸º${episode_reward}")
                flag = True if episode_reward >= 195 else False
                self.done_rate.append(flag)
                break

            observation = next_observation
        return episode_reward


class CartPole(VPGAgent, OffPolicyVPGAgent):
    def __init__(self):
        VPGAgent.__init__(self)
        OffPolicyVPGAgent.__init__(self)
        self.class_name = self.__class__.__name__

    def game_iteration(self, show_policy):
        """
        è¿­ä»£
        :param show_policy: ä½¿ç”¨çš„æ›´æ–°ç­–ç•¥æ–¹å¼
        """
        episode_reward = 0.
        episode_rewards = []  # æ€»è½®æ•°çš„å¥–åŠ±(æŸè½®æ€»å¥–åŠ±)åˆ—è¡¨
        logger.info(f"*****å¯åŠ¨: {show_policy}*****")
        method_name = "default"
        for game_round in range(1, self.game_rounds):
            logger.info(f"---ç¬¬{game_round}è½®è®­ç»ƒ---")

            if show_policy == "åŒç­–ç­–ç•¥æ¢¯åº¦ç®—æ³•":
                # logger.info(f"å‡½æ•°è¿‘ä¼¼SARSAç®—æ³•")
                episode_reward = self.play_montecarlo(train=True)  # ç¬¬roundè½®æ¬¡çš„ç´¯ç§¯reward
                method_name = self.play_montecarlo.__name__

            if show_policy == "å¼‚ç­–ç­–ç•¥æ¢¯åº¦ç®—æ³•":
                # logger.info(f"å‡½æ•°è¿‘ä¼¼SARSAç®—æ³•")
                episode_reward = self.off_play_montecarlo(train=False)  # ç¬¬roundè½®æ¬¡çš„ç´¯ç§¯reward
                method_name = self.off_play_montecarlo.__name__

            if self.global_is_train and self.save_policy and (
                    game_round % 150 == 0 or game_round == self.game_rounds - 1):
                if show_policy == "åŒç­–ç­–ç•¥æ¢¯åº¦ç®—æ³•":
                    save_data = {"policy_net": self.policy_net,
                                 "baseline_net": self.baseline_net,
                                 "policy_optimizer": self.policy_optimizer,
                                 "baseline_optimizer": self.baseline_optimizer}
                    Policy_loader.save_policy(method_name, self.class_name, save_data, step=game_round)
                if show_policy == "å¼‚ç­–ç­–ç•¥æ¢¯åº¦ç®—æ³•":
                    save_data = {"off_policy_net": self.off_policy_net,
                                 "off_baseline_net": self.off_baseline_net,
                                 "off_policy_optimizer": self.off_policy_optimizer,
                                 "off_baseline_optimizer": self.off_baseline_optimizer}
                    Policy_loader.save_policy(method_name, self.class_name, save_data, step=game_round)

            if episode_reward is not None:
                episode_rewards.append(episode_reward)
                if self.is_open_writer:
                    if self.learn_step_counter % 10 == 0:  # æ¯ 10 è½®è®°å½•ä¸€æ¬¡å¥–åŠ±
                        self.writer.add_scalar("Episode Reward", episode_reward, global_step=self.learn_step_counter)
                if self.global_is_train:
                    if False not in self.done_rate and np.round(np.mean(episode_rewards[-100:]),
                                                                2) >= 197 and self.global_is_train:
                        logger.info(f"!!!æˆåŠŸç‡å·²ç»è¾¾åˆ°ç™¾å›åˆ195ï¼Œè‡ªåŠ¨åœæ­¢è®­ç»ƒ!!!")
                        break
            else:
                logger.warning(f"ç¬¬{game_round}è½®å¥–åŠ±ä¸º Noneï¼Œå·²è·³è¿‡ã€‚")

            Visualizer.plot_cumulative_avg_rewards(episode_rewards, game_round, self.game_rounds, self.class_name,
                                                   method_name)

        print(
            f"å¹³å‡å¥–åŠ±ï¼š{(np.round(np.mean(episode_rewards), 2))} = {np.sum(episode_rewards)} / {len(episode_rewards)}")
        print(
            f"æœ€å100è½®å¥–åŠ±ï¼š{(np.round(np.mean(episode_rewards[-100:]), 2))} = {np.sum(episode_rewards[-100:])} / {len(episode_rewards[-100:])}")
        logger.info(f"*****ç»“æŸ: {show_policy}*****")
